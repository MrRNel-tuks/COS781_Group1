{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MrRNel-tuks/COS781_Group1/blob/main/COS781_Group1_Ruan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project Overview and Usage\n",
        "\n",
        "This notebook hosts the COS781 Group 1 study on accelerating association-rule mining for the Instacart transaction corpus. The workflow compares three pipelines‚Äîbaseline Apriori, QR-reduced Apriori, and a hybrid QR+DHP variant‚Äîto determine how dimensionality reduction and candidate pruning affect runtime, memory, and rule quality.\n",
        "\n",
        "### What the Project Demonstrates\n",
        "- Baseline Apriori establishes the reference cost of mining rules on the full one-hot transaction matrix.\n",
        "- QR decomposition removes redundant product columns so Apriori explores a smaller item space.\n",
        "- Direct Hashing and Pruning (DHP) limits candidate generation and prunes infrequent transactions early.\n",
        "- The recorded metrics (runtime, RAM usage, rule count, average lift/confidence) quantify efficiency gains and any accuracy trade-offs.\n",
        "\n",
        "### Optimization Techniques in This Project\n",
        "- **Sparse baskets**: Transactions are stored in SciPy CSR matrices, keeping only non-zero entries and cutting memory by orders of magnitude while enabling fast matrix ops.\n",
        "- **Hybrid dimensionality reduction**: QR column selection followed by optional PCA/SVD filters the feature space before Apriori explores it.\n",
        "- **Parallel workflows**: Heavy steps‚ÄîSVD, customer scoring, and sparse filtering‚Äîinvoke `joblib.Parallel` or multi-threaded BLAS so all CPU cores contribute.\n",
        "- **Candidate pruning**: DHP hashing trims low-frequency pairs early, shrinking each Apriori level.\n",
        "- **Streaming metrics**: Runtime and RAM tracking run alongside mining to capture cost/benefit for every configuration.\n",
        "\n",
        "### How to Use This Notebook\n",
        "1. **Environment setup**: Install the dependencies listed below (Colab or local) and ensure the Instacart CSVs plus `kaggle.json` are accessible.\n",
        "2. **Data preparation**: Run the preparation cells to load, merge, and one-hot encode the orders ‚Üí products mapping.\n",
        "3. **Choose experiments**: Execute the baseline, QR-only, and QR+DHP experiment runners. Adjust the QR threshold `gamma` values, sparsity filters, parallel worker counts, or DHP parameters as needed.\n",
        "4. **Review outputs**: Inspect the metrics table and plots to compare methods, then export the results for integration into the final report.\n",
        "\n",
        "### Resource Constraints & Sampling\n",
        "- **Default 20‚ÄØ000-order slice**: To stay within workstation RAM/CPU limits, all experiment modes currently run on a stratified sample. The pipeline can ingest the full dataset, but doing so requires cloud/GPU-class memory.\n",
        "- **Baseline vs. optimized**: Baseline Apriori uses that same 20‚ÄØ000 slice to make speedups directly comparable; QR-only and QR+DHP operate on identical rows before applying their reductions.\n",
        "- **Interpretation**: Treat the reported gains as conservative. With more memory, the same sparse + parallel tooling can extend the optimized pipelines to the full Instacart matrix.\n",
        "\n"
      ],
      "metadata": {
        "id": "fRDi-_uacVGc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß† The Big Picture\n",
        "\n",
        "We treat the Instacart market-basket matrix as a massive, sparse signal. By compressing it with QR-driven column selection, pruning it with DHP hashing, and executing every heavy stage (sparse construction, SVD, rule mining) with multi-threaded primitives, we turn classical Apriori into a scalable analytics pipeline. The goal is to surface practically useful association rules while keeping runtime and memory proportional to the true number of purchases‚Äînot to the tens of thousands of potential products.\n"
      ],
      "metadata": {
        "id": "QllF6-7vh8G5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbpq7TdY72Sj",
        "outputId": "18efcad6-4aa0-4308-ac39-8d4be873c056"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "v7gydeFX3cOE"
      },
      "outputs": [],
      "source": [
        "#disable annoying warnings\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\".*utcnow.*\", category=DeprecationWarning)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBsZiUBzioVE",
        "outputId": "c3f8ffce-ed2b-4cf2-8827-11ab12723180"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîπ Environment detected: Google Colab\n",
            "üîπ Creating kaggle.json automatically...\n",
            "‚úÖ kaggle.json created successfully at /root/.kaggle/kaggle.json for Google Colab\n",
            "üîπ Setting up Google Colab environment...\n",
            "üîπ Configuring Kaggle credentials for Colab...\n",
            "‚úÖ Kaggle credentials found and permissions set successfully at ~/.kaggle/kaggle.json.\n",
            "üîπ Loading data from KaggleHub dataset: yasserh/instacart-online-grocery-basket-analysis-dataset...\n",
            "   - Loading orders.csv...\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/yasserh/instacart-online-grocery-basket-analysis-dataset?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 197M/197M [00:07<00:00, 29.4MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   - Loaded 3421083 orders.\n",
            "   - Loading order_products__prior.csv...\n",
            "   - Loaded 32434489 order-product records.\n",
            "   - Loading products.csv...\n",
            "   - Loaded 49688 products.\n",
            "üîπ Merging datasets...\n",
            "   - Merged dataset has 32434489 records.\n",
            "üîπ Applying minimal data cleansing (preserving all products for rule analysis)...\n",
            "   - Removed 156748 orders with < 2 products\n",
            "   - Kept 3058126 orders with >= 2 products\n",
            "   - Preserved ALL 49674 products for association rule analysis\n",
            "   - Data cleansing complete - all products and quantities preserved for rule analysis\n",
            "   - Applying customer stratified sampling for 20000 customers...\n",
            "   - Available customers after cleansing: 3058126\n",
            "   - Customer stratified sampling completed:\n",
            "     * High-activity customers: 5342\n",
            "     * Medium-activity customers: 10629\n",
            "     * Low-activity customers: 4029\n",
            "     * Total selected: 20000 customers\n",
            "   - Final dataset: 210693 records, 20000 customers, 21944 products\n",
            "üîπ Creating basket matrix for association rule analysis...\n",
            "   - Dataset has 20000 orders and 21944 products\n",
            "   - Estimated full matrix size: 438,880,000 cells\n",
            "   - Creating sparse basket matrix with all products preserved...\n",
            "   - Creating sparse basket matrix: 20000 orders √ó 21944 products\n",
            "   - Filling sparse matrix...\n",
            "   - Sparse basket matrix created: 20000 orders √ó 21944 products\n",
            "   - Matrix density: 0.0005\n",
            "   - Memory efficient: Only stores 210,693 non-zero values\n",
            "\n",
            "‚úÖ Data loading and cleansing completed successfully!\n",
            "Final DataFrame shape: 20000 orders √ó 21944 products\n",
            "üîπ Analyzing data quality for Apriori...\n",
            "   - Total orders: 20,000\n",
            "   - Total products: 21,944\n",
            "   - Total transactions: 210,693\n",
            "   - Average basket size: 10.53 products per order\n",
            "   - Most frequent product: 2999 occurrences\n",
            "   - Least frequent product: 1 occurrences\n",
            "   - Products with >100 occurrences: 266\n",
            "   - Largest basket: 102 products\n",
            "   - Smallest basket: 2 products\n",
            "   - Orders with >5 products: 14431\n",
            "   - Matrix sparsity: 99.95%\n",
            "\n",
            "üîπ Applying customer-based PCA to identify important customer segments...\n",
            "üîπ Running PCA on customers (orders) using sparse matrix operations...\n",
            "   - Converting sparse matrix to CSR format for faster operations...\n",
            "   - Estimating optimal number of components...\n",
            "   - Using 1 components (95.0% variance explained)\n",
            "   - Computing SVD on full dataset using 8 parallel workers (this may take several minutes)...\n",
            "   - Note: TruncatedSVD uses optimized BLAS operations (already parallelized)\n",
            "   - SVD computation completed\n",
            "   - Computing customer importance scores (parallelized with 8 workers)...\n",
            "   - Important customers: 16333 / 20000 (81.66%)\n",
            "   - Filtering sparse matrix (parallelized with 8 workers)...\n",
            "   - Filtered shape: (16333, 21944)\n",
            "\n",
            "‚úÖ DataFrames ready for next function block!\n",
            "   - basket_df: Complete basket matrix (all customers, all products)\n",
            "   - filtered_basket_df: PCA-filtered dataset (important customers only)\n",
            "   - important_customers: List of important customer IDs\n",
            "   - Ready for Apriori algorithm with optimized customer segments\n",
            "\n",
            "üéØ Dataset ready for Apriori analysis!\n",
            "   - Full dataset: All products preserved for complete association rule mining\n",
            "   - Cleaned data: Removed noise and low-frequency items\n",
            "   - Quality optimized: Suitable for support, confidence, and lift calculations\n",
            "\n",
            "üìã Data preprocessing completed! Ready for next function block:\n",
            "   - basket_df: Complete basket matrix (all customers, all products)\n",
            "   - filtered_basket_df: PCA-optimized dataset (important customers only)\n",
            "   - All quantities preserved for quantity-based association rules\n",
            "   - No product filtering to preserve all possible associations\n",
            "\n",
            "üéØ Next function block in Colab should:\n",
            "   1. Use filtered_basket_df for Apriori (RECOMMENDED - optimized customers)\n",
            "   2. Or use basket_df for complete analysis (all customers)\n",
            "   3. Implement Apriori algorithm on chosen dataset\n",
            "   4. Apply QR decomposition during Apriori execution\n",
            "   5. Implement DHP algorithm for optimization\n",
            "   6. Compare performance across methods\n",
            "\n",
            "üí° RECOMMENDATION: Start with filtered_basket_df for faster, optimized results!\n"
          ]
        }
      ],
      "source": [
        "# ==========================================================\n",
        "#  ENVIRONMENT SETUP + DATA PREPARATION (kagglehub) - Optimized\n",
        "# ==========================================================\n",
        "#\n",
        "# This script works in both Google Colab and Windows local environments.\n",
        "#\n",
        "# For Google Colab:\n",
        "# 1. Upload your kaggle.json to the Colab environment\n",
        "# 2. Run: !pip install mlxtend psutil kagglehub --quiet\n",
        "# 3. Run this script\n",
        "#\n",
        "# For Windows Local:\n",
        "# 1. Ensure kaggle.json is in the same directory as this script\n",
        "# 2. Run: pip install kagglehub\n",
        "# 3. Run this script\n",
        "#\n",
        "# ==========================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import warnings\n",
        "import shutil\n",
        "import json\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "from sklearn.decomposition import PCA, TruncatedSVD\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.sparse import csr_matrix, vstack\n",
        "from joblib import Parallel, delayed\n",
        "from multiprocessing import cpu_count\n",
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "\n",
        "# --- Configuration ---\n",
        "# Customer sampling strategy\n",
        "# Memory-safe recommendations for incremental QR (51 GB RAM limit):\n",
        "# - 10,000 customers: ~2-3 GB memory (very safe)\n",
        "# - 20,000 customers: ~5-7 GB memory (safe)\n",
        "# - 30,000 customers: ~10-12 GB memory (should be safe)\n",
        "# - 50,000 customers: ~25-30 GB memory (approaching limit)\n",
        "# Note: With incremental QR, memory scales with n_customers^2 for Q vectors\n",
        "# Set to None to use all customers (not recommended for very large datasets)\n",
        "TOP_N_CUSTOMERS =20000  # Use all customers (None = all, or set number like 2000 for stratified sampling)\n",
        "\n",
        "# Filter out specific warnings for cleaner output\n",
        "warnings.filterwarnings(\n",
        "    \"ignore\",\n",
        "    message=r\".*utcnow\\\\(\\\\) is deprecated.*\",\n",
        "    module=\"jupyter_client\",\n",
        "    category=DeprecationWarning\n",
        ")\n",
        "warnings.filterwarnings(\n",
        "    \"ignore\",\n",
        "    message=r\".*Use dataset_load\\\\(\\\\) instead of load_dataset\\\\(\\\\).*\",\n",
        "    category=DeprecationWarning\n",
        ")\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# ENVIRONMENT DETECTION\n",
        "# ----------------------------------------------------------------\n",
        "def is_google_colab():\n",
        "    \"\"\"Detect if running in Google Colab environment\"\"\"\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "# Detect environment\n",
        "IS_COLAB = is_google_colab()\n",
        "print(f\"üîπ Environment detected: {'Google Colab' if IS_COLAB else 'Local Windows'}\")\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# KAGGLE.JSON AUTO-CREATION\n",
        "# ----------------------------------------------------------------\n",
        "def create_kaggle_json():\n",
        "    \"\"\"Automatically create kaggle.json file with hardcoded credentials for both environments\"\"\"\n",
        "    kaggle_credentials = {\n",
        "        \"username\": \"mrruannel\",\n",
        "        \"key\": \"c92515c29494efcee2c5b55ae6227f43\"\n",
        "    }\n",
        "\n",
        "    if IS_COLAB:\n",
        "        # Google Colab: Create in /root/.kaggle/kaggle.json\n",
        "        kaggle_dir = Path('/root/.kaggle')\n",
        "        kaggle_dir.mkdir(exist_ok=True)\n",
        "        kaggle_json_path = kaggle_dir / 'kaggle.json'\n",
        "\n",
        "        with open(kaggle_json_path, 'w') as f:\n",
        "            json.dump(kaggle_credentials, f, indent=4)\n",
        "        os.chmod(kaggle_json_path, 0o600)\n",
        "        print(f\"‚úÖ kaggle.json created successfully at {kaggle_json_path} for Google Colab\")\n",
        "\n",
        "    else:\n",
        "        # Windows Local: Create in script directory (will be copied to ~/.kaggle later)\n",
        "        script_dir = Path(__file__).parent if '__file__' in globals() else Path.cwd()\n",
        "        kaggle_json_path = script_dir / 'kaggle.json'\n",
        "\n",
        "        with open(kaggle_json_path, 'w') as f:\n",
        "            json.dump(kaggle_credentials, f, indent=4)\n",
        "        print(f\"‚úÖ kaggle.json created successfully at {kaggle_json_path} for Windows Local\")\n",
        "\n",
        "    return True\n",
        "\n",
        "# Create kaggle.json automatically\n",
        "print(\"üîπ Creating kaggle.json automatically...\")\n",
        "create_kaggle_json()\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# ENVIRONMENT-SPECIFIC SETUP\n",
        "# ----------------------------------------------------------------\n",
        "if IS_COLAB:\n",
        "    # Google Colab specific setup\n",
        "    print(\"üîπ Setting up Google Colab environment...\")\n",
        "    # Note: In Colab, you'll need to run this cell with !pip install mlxtend psutil kagglehub --quiet\n",
        "    # This is a placeholder - the actual pip install should be run in a separate Colab cell\n",
        "\n",
        "    # Colab Kaggle authentication\n",
        "    KAGGLE_FILE_PATH = '/root/.kaggle/kaggle.json'\n",
        "    print(\"üîπ Configuring Kaggle credentials for Colab...\")\n",
        "    try:\n",
        "        if os.path.exists(KAGGLE_FILE_PATH):\n",
        "            os.chmod(KAGGLE_FILE_PATH, 0o600)\n",
        "            print(\"‚úÖ Kaggle credentials found and permissions set successfully at ~/.kaggle/kaggle.json.\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è Warning: 'kaggle.json' not found at ~/.kaggle/. Please ensure it has been uploaded or moved.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è An error occurred during Kaggle credential setup: {e}\")\n",
        "\n",
        "else:\n",
        "    # Windows local environment setup\n",
        "    print(\"üîπ Setting up Windows local environment...\")\n",
        "    print(\"‚úÖ kagglehub available\")\n",
        "\n",
        "    # Windows Kaggle authentication\n",
        "    script_dir = Path(__file__).parent if '__file__' in globals() else Path.cwd()\n",
        "    kaggle_json_path = script_dir / 'kaggle.json'\n",
        "    kaggle_dir = Path.home() / '.kaggle'\n",
        "    kaggle_dir.mkdir(exist_ok=True)\n",
        "    kaggle_file_path = kaggle_dir / 'kaggle.json'\n",
        "\n",
        "    print(\"üîπ Configuring Kaggle credentials for Windows...\")\n",
        "    try:\n",
        "        if kaggle_json_path.exists():\n",
        "            # Copy kaggle.json to the correct location\n",
        "            shutil.copy2(kaggle_json_path, kaggle_file_path)\n",
        "            os.chmod(kaggle_file_path, 0o600)\n",
        "            print(f\"‚úÖ Kaggle credentials copied from {kaggle_json_path} to {kaggle_file_path}\")\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Warning: 'kaggle.json' not found at {kaggle_json_path}\")\n",
        "            print(\"   Please ensure kaggle.json is in the same directory as this script.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è An error occurred during Kaggle credential setup: {e}\")\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "\n",
        "def load_instacart_from_kagglehub(top_n_customers=TOP_N_CUSTOMERS):\n",
        "    \"\"\"\n",
        "    Loads Instacart data from Kaggle using kagglehub, performs a memory-safe\n",
        "    filter on the number of orders, and creates the transaction basket.\n",
        "    \"\"\"\n",
        "    DATASET_ID = \"yasserh/instacart-online-grocery-basket-analysis-dataset\"\n",
        "    print(f\"üîπ Loading data from KaggleHub dataset: {DATASET_ID}...\")\n",
        "\n",
        "    # Helper function to load CSV with caching\n",
        "    def load_csv_with_encoding_fallback(dataset_id, filename):\n",
        "        \"\"\"Load CSV file with caching for faster subsequent runs\"\"\"\n",
        "        # Download dataset and load CSV file\n",
        "        dataset_path = kagglehub.dataset_download(dataset_id)\n",
        "        file_path = os.path.join(dataset_path, filename)\n",
        "        return pd.read_csv(file_path)\n",
        "\n",
        "    # Helper function to load CSV without caching\n",
        "    def load_csv_with_retry(dataset_id, filename):\n",
        "        \"\"\"Load CSV with fresh download each time\"\"\"\n",
        "        return load_csv_with_encoding_fallback(dataset_id, filename)\n",
        "\n",
        "    print(\"   - Loading orders.csv...\")\n",
        "    orders = load_csv_with_retry(DATASET_ID, \"orders.csv\")\n",
        "    print(f\"   - Loaded {len(orders)} orders.\")\n",
        "\n",
        "    print(\"   - Loading order_products__prior.csv...\")\n",
        "    order_products = load_csv_with_retry(DATASET_ID, \"order_products__prior.csv\")\n",
        "    print(f\"   - Loaded {len(order_products)} order-product records.\")\n",
        "\n",
        "    print(\"   - Loading products.csv...\")\n",
        "    products = load_csv_with_retry(DATASET_ID, \"products.csv\")\n",
        "    print(f\"   - Loaded {len(products)} products.\")\n",
        "\n",
        "    print(\"üîπ Merging datasets...\")\n",
        "    # 1. Merge order_products and products (to get product names)\n",
        "    merged = order_products.merge(products, on=\"product_id\", how=\"left\")\n",
        "    # 2. Add the order information\n",
        "    merged = merged.merge(orders[['order_id']], on=\"order_id\", how=\"left\")\n",
        "    merged = merged.dropna(subset=['product_name'])\n",
        "    print(f\"   - Merged dataset has {len(merged)} records.\")\n",
        "\n",
        "\n",
        "    print(f\"üîπ Applying minimal data cleansing (preserving all products for rule analysis)...\")\n",
        "\n",
        "    # 1. Remove orders with very few products (basket size filtering only)\n",
        "    # This is safe as it removes entire orders, not individual products\n",
        "    order_sizes = merged.groupby(\"order_id\").size()\n",
        "    min_basket_size = 2  # Orders must have at least 2 products\n",
        "    valid_orders = order_sizes[order_sizes >= min_basket_size].index\n",
        "    merged = merged[merged[\"order_id\"].isin(valid_orders)]\n",
        "    print(f\"   - Removed {len(order_sizes) - len(valid_orders)} orders with < {min_basket_size} products\")\n",
        "    print(f\"   - Kept {len(valid_orders)} orders with >= {min_basket_size} products\")\n",
        "    print(f\"   - Preserved ALL {len(merged['product_name'].unique())} products for association rule analysis\")\n",
        "\n",
        "    # 2. Data cleansing complete - all products and quantities preserved\n",
        "    print(f\"   - Data cleansing complete - all products and quantities preserved for rule analysis\")\n",
        "\n",
        "    # 3. Customer stratified sampling for representative dataset (if specified)\n",
        "    if top_n_customers is not None:\n",
        "        print(f\"   - Applying customer stratified sampling for {top_n_customers} customers...\")\n",
        "\n",
        "        # Get customer frequencies (number of products per customer)\n",
        "        customer_counts = merged.groupby('order_id').size()\n",
        "        total_customers = len(customer_counts)\n",
        "\n",
        "        print(f\"   - Available customers after cleansing: {total_customers}\")\n",
        "\n",
        "        if total_customers <= top_n_customers:\n",
        "            # If we have fewer customers than requested, use all\n",
        "            selected_customers = customer_counts.index.tolist()\n",
        "            print(f\"   - Using all {total_customers} customers (less than requested {top_n_customers})\")\n",
        "        else:\n",
        "            # Create frequency-based strata for customers\n",
        "            customer_freqs = customer_counts.values\n",
        "            customer_names = customer_counts.index.tolist()\n",
        "\n",
        "            # Define strata based on frequency percentiles\n",
        "            freq_25 = np.percentile(customer_freqs, 25)\n",
        "            freq_75 = np.percentile(customer_freqs, 75)\n",
        "\n",
        "            # Create strata labels\n",
        "            strata_labels = []\n",
        "            for freq in customer_freqs:\n",
        "                if freq >= freq_75:\n",
        "                    strata_labels.append('high')      # Top 25% (customers with many products)\n",
        "                elif freq >= freq_25:\n",
        "                    strata_labels.append('medium')    # Middle 50%\n",
        "                else:\n",
        "                    strata_labels.append('low')       # Bottom 25% (customers with few products)\n",
        "\n",
        "            # Convert to numpy arrays for sklearn\n",
        "            customer_names_array = np.array(customer_names)\n",
        "            strata_labels_array = np.array(strata_labels)\n",
        "\n",
        "            # Stratified sampling using train_test_split\n",
        "            selected_indices, _ = train_test_split(\n",
        "                range(len(customer_names)),\n",
        "                train_size=top_n_customers,\n",
        "                stratify=strata_labels_array,\n",
        "                random_state=42\n",
        "            )\n",
        "\n",
        "            selected_customers = [customer_names[i] for i in selected_indices]\n",
        "\n",
        "            # Count customers in each stratum\n",
        "            selected_strata = [strata_labels[i] for i in selected_indices]\n",
        "            high_count = selected_strata.count('high')\n",
        "            medium_count = selected_strata.count('medium')\n",
        "            low_count = selected_strata.count('low')\n",
        "\n",
        "            print(f\"   - Customer stratified sampling completed:\")\n",
        "            print(f\"     * High-activity customers: {high_count}\")\n",
        "            print(f\"     * Medium-activity customers: {medium_count}\")\n",
        "            print(f\"     * Low-activity customers: {low_count}\")\n",
        "            print(f\"     * Total selected: {len(selected_customers)} customers\")\n",
        "\n",
        "        # Filter merged data to keep only selected customers\n",
        "        merged = merged[merged['order_id'].isin(selected_customers)]\n",
        "        print(f\"   - Final dataset: {len(merged)} records, {len(selected_customers)} customers, {len(merged['product_name'].unique())} products\")\n",
        "    else:\n",
        "        print(f\"   - Using all customers after cleansing...\")\n",
        "        unique_customers = merged['order_id'].unique()\n",
        "        unique_products = merged['product_name'].unique()\n",
        "        print(f\"   - Final dataset: {len(merged)} records, {len(unique_customers)} customers, {len(unique_products)} products\")\n",
        "\n",
        "    print(\"üîπ Creating basket matrix for association rule analysis...\")\n",
        "    print(f\"   - Dataset has {len(merged['order_id'].unique())} orders and {len(merged['product_name'].unique())} products\")\n",
        "    print(f\"   - Estimated full matrix size: {len(merged['order_id'].unique()) * len(merged['product_name'].unique()):,} cells\")\n",
        "    print(f\"   - Creating sparse basket matrix with all products preserved...\")\n",
        "\n",
        "    # Create basket matrix with all products (no filtering)\n",
        "    orders = merged['order_id'].unique()\n",
        "    products = merged['product_name'].unique()\n",
        "\n",
        "    # Create mapping dictionaries\n",
        "    order_to_idx = {order: idx for idx, order in enumerate(orders)}\n",
        "    product_to_idx = {product: idx for idx, product in enumerate(products)}\n",
        "\n",
        "    print(f\"   - Creating sparse basket matrix: {len(orders)} orders √ó {len(products)} products\")\n",
        "\n",
        "    # Build sparse matrix data\n",
        "    row_indices = []\n",
        "    col_indices = []\n",
        "\n",
        "    print(\"   - Filling sparse matrix...\")\n",
        "    for order_id, group in merged.groupby('order_id'):\n",
        "        order_idx = order_to_idx[order_id]\n",
        "        for product_name in group['product_name']:\n",
        "            product_idx = product_to_idx[product_name]\n",
        "            row_indices.append(order_idx)\n",
        "            col_indices.append(product_idx)\n",
        "\n",
        "    # Create sparse matrix\n",
        "    data = np.ones(len(row_indices), dtype=np.int8)  # Use int8 to save memory\n",
        "\n",
        "    # Use numpy int64 for shape to avoid C long overflow\n",
        "    shape = (np.int64(len(orders)), np.int64(len(products)))\n",
        "    basket_sparse = csr_matrix((data, (row_indices, col_indices)),\n",
        "                               shape=shape)\n",
        "\n",
        "    # Convert to DataFrame for compatibility (but keep it sparse-aware)\n",
        "    basket = pd.DataFrame.sparse.from_spmatrix(basket_sparse,\n",
        "                                               index=orders,\n",
        "                                               columns=products)\n",
        "\n",
        "    print(f\"   - Sparse basket matrix created: {basket.shape[0]} orders √ó {basket.shape[1]} products\")\n",
        "\n",
        "    # Use float64 for density calculation to avoid overflow\n",
        "    total_cells = float(basket.shape[0]) * float(basket.shape[1])\n",
        "    density = basket_sparse.nnz / total_cells\n",
        "    print(f\"   - Matrix density: {density:.4f}\")\n",
        "    print(f\"   - Memory efficient: Only stores {basket_sparse.nnz:,} non-zero values\")\n",
        "\n",
        "    return basket\n",
        "\n",
        "# ==========================================================\n",
        "# CUSTOMER-BASED PCA FOR SEGMENTATION\n",
        "# ==========================================================\n",
        "\n",
        "def apply_customer_pca(basket_df, n_components=None, explained_variance_threshold=0.95, customer_importance_threshold=0.01, n_jobs=-1):\n",
        "    \"\"\"\n",
        "    Apply PCA on customers (orders) using sparse matrix operations with parallel processing.\n",
        "\n",
        "    Parameters:\n",
        "        basket_df: Sparse DataFrame with customers as rows, products as columns\n",
        "        n_components: Number of components (None = auto-determine)\n",
        "        explained_variance_threshold: Variance threshold for auto-determining components\n",
        "        customer_importance_threshold: Threshold for filtering important customers\n",
        "        n_jobs: Number of parallel workers (-1 = use all cores)\n",
        "    \"\"\"\n",
        "    print(\"üîπ Running PCA on customers (orders) using sparse matrix operations...\")\n",
        "\n",
        "    if n_jobs == -1:\n",
        "        n_jobs = cpu_count()\n",
        "\n",
        "    # Get underlying sparse matrix and convert to CSR for faster operations\n",
        "    print(\"   - Converting sparse matrix to CSR format for faster operations...\")\n",
        "    sparse_matrix = basket_df.sparse.to_coo().tocsr()  # CSR is faster for row operations\n",
        "    customer_names = basket_df.index.tolist()\n",
        "    customer_names_dict = {name: idx for idx, name in enumerate(customer_names)}  # For faster lookup\n",
        "\n",
        "    # Determine number of components if not specified\n",
        "    if n_components is None:\n",
        "        print(\"   - Estimating optimal number of components...\")\n",
        "        temp_components = min(100, min(sparse_matrix.shape) - 1)\n",
        "        svd_temp = TruncatedSVD(n_components=temp_components, random_state=42)\n",
        "        svd_temp.fit(sparse_matrix)\n",
        "        cumsum = np.cumsum(svd_temp.explained_variance_ratio_)\n",
        "        n_components = np.argmax(cumsum >= explained_variance_threshold) + 1\n",
        "        n_components = min(n_components, min(sparse_matrix.shape) - 1)\n",
        "        print(f\"   - Using {n_components} components ({explained_variance_threshold*100}% variance explained)\")\n",
        "\n",
        "    # Use TruncatedSVD for sparse matrices (memory efficient)\n",
        "    # Note: sklearn's TruncatedSVD uses optimized BLAS which is already parallelized\n",
        "    print(f\"   - Computing SVD on full dataset using {n_jobs} parallel workers (this may take several minutes)...\")\n",
        "    print(f\"   - Note: TruncatedSVD uses optimized BLAS operations (already parallelized)\")\n",
        "    svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
        "    X_transformed = svd.fit_transform(sparse_matrix)\n",
        "    print(\"   - SVD computation completed\")\n",
        "\n",
        "    # Compute importance per customer (based on SVD scores) - can be parallelized\n",
        "    print(f\"   - Computing customer importance scores (parallelized with {n_jobs} workers)...\")\n",
        "\n",
        "    # Parallelize norm computation for large datasets\n",
        "    n_customers = X_transformed.shape[0]\n",
        "    chunk_size = max(1000, n_customers // (n_jobs * 4))  # Divide into chunks\n",
        "\n",
        "    def compute_norm_chunk(chunk_data):\n",
        "        \"\"\"Compute norms for a chunk of transformed data\"\"\"\n",
        "        return np.linalg.norm(chunk_data, axis=1)\n",
        "\n",
        "    # Process in parallel chunks\n",
        "    if n_customers > 10000:  # Only parallelize for large datasets\n",
        "        chunks = [X_transformed[i:i+chunk_size] for i in range(0, n_customers, chunk_size)]\n",
        "        norm_chunks = Parallel(n_jobs=n_jobs, backend='threading', verbose=0)(\n",
        "            delayed(compute_norm_chunk)(chunk) for chunk in chunks\n",
        "        )\n",
        "        customer_importance = np.concatenate(norm_chunks)\n",
        "    else:\n",
        "        # For smaller datasets, use direct computation\n",
        "        customer_importance = np.linalg.norm(X_transformed, axis=1)\n",
        "\n",
        "    customer_importance /= np.max(customer_importance)  # Normalize 0‚Äì1\n",
        "\n",
        "    important_mask = customer_importance >= customer_importance_threshold\n",
        "    important_customers = np.array(customer_names)[important_mask]\n",
        "\n",
        "    print(f\"   - Important customers: {len(important_customers)} / {len(customer_names)} ({important_mask.mean():.2%})\")\n",
        "\n",
        "    # Filter sparse matrix directly (much faster than DataFrame.loc)\n",
        "    print(f\"   - Filtering sparse matrix (parallelized with {n_jobs} workers)...\")\n",
        "    important_indices = np.array([customer_names_dict[cust] for cust in important_customers])\n",
        "\n",
        "    # Use parallel indexing for large filtering operations\n",
        "    if len(important_indices) > 10000:\n",
        "        # For large filtering, process in chunks\n",
        "        chunk_size = max(1000, len(important_indices) // (n_jobs * 4))\n",
        "        chunks = [important_indices[i:i+chunk_size] for i in range(0, len(important_indices), chunk_size)]\n",
        "\n",
        "        def filter_chunk(indices_chunk):\n",
        "            \"\"\"Filter sparse matrix for a chunk of indices\"\"\"\n",
        "            return sparse_matrix[indices_chunk, :]\n",
        "\n",
        "        filtered_chunks = Parallel(n_jobs=n_jobs, backend='threading', verbose=0)(\n",
        "            delayed(filter_chunk)(chunk) for chunk in chunks\n",
        "        )\n",
        "\n",
        "        # Combine chunks\n",
        "        filtered_sparse = vstack(filtered_chunks)\n",
        "    else:\n",
        "        # For smaller filtering, use direct indexing\n",
        "        filtered_sparse = sparse_matrix[important_indices, :]\n",
        "\n",
        "    # Convert back to DataFrame\n",
        "    filtered_basket_df = pd.DataFrame.sparse.from_spmatrix(\n",
        "        filtered_sparse,\n",
        "        index=important_customers,\n",
        "        columns=basket_df.columns\n",
        "    )\n",
        "    print(f\"   - Filtered shape: {filtered_basket_df.shape}\")\n",
        "\n",
        "    return filtered_basket_df, svd, customer_importance, important_customers\n",
        "\n",
        "\n",
        "def reconstruct_customers_from_pca(customer_pca_df, pca_model):\n",
        "    \"\"\"\n",
        "    Reconstruct original customer basket matrix from PCA components.\n",
        "    Useful for interpreting customer segments back to original order space.\n",
        "    \"\"\"\n",
        "    if pca_model is None:\n",
        "        return customer_pca_df\n",
        "\n",
        "    print(\"üîπ Reconstructing original customer basket matrix from PCA components...\")\n",
        "    reconstructed = pca_model.inverse_transform(customer_pca_df.values)\n",
        "\n",
        "    # Create DataFrame with original product names\n",
        "    reconstructed_df = pd.DataFrame(\n",
        "        reconstructed,\n",
        "        index=[f'Reconstructed_Customer_{i+1}' for i in range(reconstructed.shape[0])],\n",
        "        columns=customer_pca_df.columns\n",
        "    )\n",
        "\n",
        "    print(f\"   - Reconstructed matrix shape: {reconstructed_df.shape}\")\n",
        "    return reconstructed_df\n",
        "\n",
        "# ==========================================================\n",
        "# BASKET MATRIX READY FOR APRIORI\n",
        "# ==========================================================\n",
        "\n",
        "# ==========================================================\n",
        "# DATA QUALITY ANALYSIS\n",
        "# ==========================================================\n",
        "\n",
        "def analyze_data_quality(basket_df):\n",
        "    \"\"\"\n",
        "    Analyze the quality of the basket matrix for Apriori\n",
        "    Uses sparse matrix operations to avoid overflow with large matrices\n",
        "    \"\"\"\n",
        "    print(f\"üîπ Analyzing data quality for Apriori...\")\n",
        "\n",
        "    # Get underlying sparse matrix to avoid pandas overflow issues\n",
        "    sparse_matrix = basket_df.sparse.to_coo()\n",
        "\n",
        "    # Basic statistics\n",
        "    total_orders = int(basket_df.shape[0])\n",
        "    total_products = int(basket_df.shape[1])\n",
        "    total_transactions = int(sparse_matrix.nnz)  # Number of non-zero elements\n",
        "\n",
        "    print(f\"   - Total orders: {total_orders:,}\")\n",
        "    print(f\"   - Total products: {total_products:,}\")\n",
        "    print(f\"   - Total transactions: {total_transactions:,}\")\n",
        "    print(f\"   - Average basket size: {total_transactions/total_orders:.2f} products per order\")\n",
        "\n",
        "    # Product frequency analysis using sparse matrix operations\n",
        "    product_frequencies = np.array(sparse_matrix.sum(axis=0)).flatten()\n",
        "    print(f\"   - Most frequent product: {int(product_frequencies.max())} occurrences\")\n",
        "    print(f\"   - Least frequent product: {int(product_frequencies.min())} occurrences\")\n",
        "    print(f\"   - Products with >100 occurrences: {int((product_frequencies > 100).sum())}\")\n",
        "\n",
        "    # Order size analysis using sparse matrix operations\n",
        "    order_sizes = np.array(sparse_matrix.sum(axis=1)).flatten()\n",
        "    print(f\"   - Largest basket: {int(order_sizes.max())} products\")\n",
        "    print(f\"   - Smallest basket: {int(order_sizes.min())} products\")\n",
        "    print(f\"   - Orders with >5 products: {int((order_sizes > 5).sum())}\")\n",
        "\n",
        "    # Sparsity analysis\n",
        "    total_cells = float(total_orders) * float(total_products)\n",
        "    sparsity = (total_cells - total_transactions) / total_cells\n",
        "    print(f\"   - Matrix sparsity: {sparsity:.2%}\")\n",
        "\n",
        "    return {\n",
        "        'total_orders': total_orders,\n",
        "        'total_products': total_products,\n",
        "        'total_transactions': total_transactions,\n",
        "        'avg_basket_size': total_transactions/total_orders,\n",
        "        'sparsity': sparsity\n",
        "    }\n",
        "\n",
        "# ==========================================================\n",
        "# Data Loading Execution\n",
        "# ==========================================================\n",
        "basket_df = load_instacart_from_kagglehub()\n",
        "\n",
        "# --- Final Confirmation and Data Quality Analysis ---\n",
        "if not basket_df.empty:\n",
        "    print(\"\\n‚úÖ Data loading and cleansing completed successfully!\")\n",
        "    print(f\"Final DataFrame shape: {basket_df.shape[0]} orders √ó {basket_df.shape[1]} products\")\n",
        "\n",
        "    # Analyze data quality for Apriori\n",
        "    quality_stats = analyze_data_quality(basket_df)\n",
        "\n",
        "    # Apply customer-based PCA to identify important customer segments\n",
        "    print(\"\\nüîπ Applying customer-based PCA to identify important customer segments...\")\n",
        "    filtered_basket_df, customer_pca_model, customer_importance, important_customers = apply_customer_pca(basket_df)\n",
        "\n",
        "    # DataFrames ready for next function block\n",
        "    print(\"\\n‚úÖ DataFrames ready for next function block!\")\n",
        "    print(\"   - basket_df: Complete basket matrix (all customers, all products)\")\n",
        "    print(\"   - filtered_basket_df: PCA-filtered dataset (important customers only)\")\n",
        "    print(\"   - important_customers: List of important customer IDs\")\n",
        "    print(\"   - Ready for Apriori algorithm with optimized customer segments\")\n",
        "\n",
        "    print(\"\\nüéØ Dataset ready for Apriori analysis!\")\n",
        "    print(\"   - Full dataset: All products preserved for complete association rule mining\")\n",
        "    print(\"   - Cleaned data: Removed noise and low-frequency items\")\n",
        "    print(\"   - Quality optimized: Suitable for support, confidence, and lift calculations\")\n",
        "    print(\"\\nüìã Data preprocessing completed! Ready for next function block:\")\n",
        "    print(\"   - basket_df: Complete basket matrix (all customers, all products)\")\n",
        "    print(\"   - filtered_basket_df: PCA-optimized dataset (important customers only)\")\n",
        "    print(\"   - All quantities preserved for quantity-based association rules\")\n",
        "    print(\"   - No product filtering to preserve all possible associations\")\n",
        "    print(\"\\nüéØ Next function block in Colab should:\")\n",
        "    print(\"   1. Use filtered_basket_df for Apriori (RECOMMENDED - optimized customers)\")\n",
        "    print(\"   2. Or use basket_df for complete analysis (all customers)\")\n",
        "    print(\"   3. Implement Apriori algorithm on chosen dataset\")\n",
        "    print(\"   4. Apply QR decomposition during Apriori execution\")\n",
        "    print(\"   5. Implement DHP algorithm for optimization\")\n",
        "    print(\"   6. Compare performance across methods\")\n",
        "    print(\"\\nüí° RECOMMENDATION: Start with filtered_basket_df for faster, optimized results!\")\n",
        "\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è Script did not complete successfully. The basket DataFrame is empty.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "id": "SNxTZugJj7y2",
        "outputId": "f9b70046-db0c-490a-b237-e8dcf9d08f04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing with filtered_basket_df (entire dataset, parallelized):\n",
            "Configuration: MIN_SUPPORT=0.01, MIN_CONFIDENCE=0.3, N_JOBS=-1\n",
            "‚ñ∂Ô∏è Running Baseline Apriori on entire dataset...\n",
            "   - Dataset shape: (16333, 21944)\n",
            "   - Using parallelized sparse-aware Apriori (processing entire dataset)...\n",
            "   - Processing 16,333 transactions, 21,944 items\n",
            "   - Support threshold: 163 transactions (1.0%)\n",
            "   - Using 8 parallel workers (threading)\n",
            "   - Progress tracking: ENABLED\n",
            "   - Level 1: Finding frequent items...\n",
            "   - Level 1: Found 144 frequent 1-itemsets (took 0.00s)\n",
            "   - Level 2: Generating candidates from 144 frequent 1-itemsets...\n",
            "   - Level 2: Checking 10,296 candidates in parallel...\n",
            "   - Level 2: Found 28 frequent 2-itemsets (checked 10,296 candidates in 17.94s, ~574 candidates/sec)\n",
            "     * Total time elapsed: 18.0s\n",
            "   - Level 3: Generating candidates from 28 frequent 2-itemsets...\n",
            "   - Level 3: Checking 100 candidates in parallel...\n",
            "   - Level 3: No frequent itemsets found (took 0.21s)\n",
            "   - Total frequent itemsets found: 172\n",
            "   - Total Apriori time: 18.20s (0.3 minutes)\n",
            "   - Generating association rules from 172 frequent itemsets...\n",
            "   - Rule generation completed in 0.02s\n",
            "\n",
            "‚úÖ COMPLETED:\n",
            "   - Total runtime: 21.9s (0.4 minutes)\n",
            "   - Memory usage: 46.88 MB\n",
            "   - Frequent itemsets: 172\n",
            "   - Association rules: 4\n",
            "\n",
            "üîπ Sample of discovered rules:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "              antecedents               consequents  antecedent support  \\\n",
              "0        (Cucumber Kirby)                  (Banana)            0.039123   \n",
              "1      (Honeycrisp Apple)                  (Banana)            0.032511   \n",
              "2  (Organic Hass Avocado)  (Bag of Organic Bananas)            0.084002   \n",
              "3    (Organic Fuji Apple)                  (Banana)            0.037103   \n",
              "\n",
              "   consequent support   support  confidence      lift  representativity  \\\n",
              "0            0.183616  0.012919    0.330203  1.798337               1.0   \n",
              "1            0.183616  0.010837    0.333333  1.815383               1.0   \n",
              "2            0.151779  0.026450    0.314869  2.074527               1.0   \n",
              "3            0.183616  0.013715    0.369637  2.013098               1.0   \n",
              "\n",
              "   leverage  conviction  zhangs_metric   jaccard  certainty  kulczynski  \n",
              "0  0.005735    1.218854       0.462006  0.061570   0.179557    0.200280  \n",
              "1  0.004867    1.224576       0.464245  0.052789   0.183391    0.196177  \n",
              "2  0.013700    1.238042       0.565462  0.126353   0.192273    0.244566  \n",
              "3  0.006902    1.295101       0.522645  0.066253   0.227860    0.222164  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-27e2cb45-ca24-4e41-9b64-2007edd82d2a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>antecedents</th>\n",
              "      <th>consequents</th>\n",
              "      <th>antecedent support</th>\n",
              "      <th>consequent support</th>\n",
              "      <th>support</th>\n",
              "      <th>confidence</th>\n",
              "      <th>lift</th>\n",
              "      <th>representativity</th>\n",
              "      <th>leverage</th>\n",
              "      <th>conviction</th>\n",
              "      <th>zhangs_metric</th>\n",
              "      <th>jaccard</th>\n",
              "      <th>certainty</th>\n",
              "      <th>kulczynski</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>(Cucumber Kirby)</td>\n",
              "      <td>(Banana)</td>\n",
              "      <td>0.039123</td>\n",
              "      <td>0.183616</td>\n",
              "      <td>0.012919</td>\n",
              "      <td>0.330203</td>\n",
              "      <td>1.798337</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.005735</td>\n",
              "      <td>1.218854</td>\n",
              "      <td>0.462006</td>\n",
              "      <td>0.061570</td>\n",
              "      <td>0.179557</td>\n",
              "      <td>0.200280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>(Honeycrisp Apple)</td>\n",
              "      <td>(Banana)</td>\n",
              "      <td>0.032511</td>\n",
              "      <td>0.183616</td>\n",
              "      <td>0.010837</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>1.815383</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.004867</td>\n",
              "      <td>1.224576</td>\n",
              "      <td>0.464245</td>\n",
              "      <td>0.052789</td>\n",
              "      <td>0.183391</td>\n",
              "      <td>0.196177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>(Organic Hass Avocado)</td>\n",
              "      <td>(Bag of Organic Bananas)</td>\n",
              "      <td>0.084002</td>\n",
              "      <td>0.151779</td>\n",
              "      <td>0.026450</td>\n",
              "      <td>0.314869</td>\n",
              "      <td>2.074527</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.013700</td>\n",
              "      <td>1.238042</td>\n",
              "      <td>0.565462</td>\n",
              "      <td>0.126353</td>\n",
              "      <td>0.192273</td>\n",
              "      <td>0.244566</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>(Organic Fuji Apple)</td>\n",
              "      <td>(Banana)</td>\n",
              "      <td>0.037103</td>\n",
              "      <td>0.183616</td>\n",
              "      <td>0.013715</td>\n",
              "      <td>0.369637</td>\n",
              "      <td>2.013098</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.006902</td>\n",
              "      <td>1.295101</td>\n",
              "      <td>0.522645</td>\n",
              "      <td>0.066253</td>\n",
              "      <td>0.227860</td>\n",
              "      <td>0.222164</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-27e2cb45-ca24-4e41-9b64-2007edd82d2a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-27e2cb45-ca24-4e41-9b64-2007edd82d2a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-27e2cb45-ca24-4e41-9b64-2007edd82d2a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-40584d94-88e0-41c9-b873-01dd7a5148e9\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-40584d94-88e0-41c9-b873-01dd7a5148e9')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-40584d94-88e0-41c9-b873-01dd7a5148e9 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"print(f\\\"\\\\n\\ud83d\\udcc8 Baseline Summary \\u2192 Runtime: {t_base}s | Memory: {m_base} MB | Rules found: {len(rules_base)}\\\")\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"antecedents\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"frozenset({'Honeycrisp Apple'})\",\n          \"frozenset({'Organic Fuji Apple'})\",\n          \"frozenset({'Cucumber Kirby'})\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"consequents\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"frozenset({'Bag of Organic Bananas'})\",\n          \"frozenset({'Banana'})\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"antecedent support\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.024037789779738517,\n        \"min\": 0.03251086756872589,\n        \"max\": 0.08400171432070042,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.03251086756872589,\n          0.03710279801628605\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"consequent support\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.015918692218208533,\n        \"min\": 0.1517786077266883,\n        \"max\": 0.18361599216310537,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.1517786077266883,\n          0.18361599216310537\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"support\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.00708439393428912,\n        \"min\": 0.010836955856241963,\n        \"max\": 0.026449519377946488,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.010836955856241963,\n          0.01371456560337966\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"confidence\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.023199126326313914,\n        \"min\": 0.31486880466472306,\n        \"max\": 0.3696369636963696,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.3333333333333333,\n          0.3696369636963696\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lift\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.13925813708122947,\n        \"min\": 1.7983370565357988,\n        \"max\": 2.0745269006006137,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1.8153829054129154,\n          2.0130978753093713\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"representativity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 1.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"leverage\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0040199132730187066,\n        \"min\": 0.0048674406515270335,\n        \"max\": 0.013699856131695564,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.0048674406515270335\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"conviction\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.034911250828715795,\n        \"min\": 1.2188536939434012,\n        \"max\": 1.2951013318040792,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1.224576011755342\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"zhangs_metric\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04989493844288721,\n        \"min\": 0.4620058862716273,\n        \"max\": 0.5654621769687608,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.4642450322743956\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"jaccard\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.03354208142809301,\n        \"min\": 0.052788547569340885,\n        \"max\": 0.1263527347177537,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.052788547569340885\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"certainty\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.022046005716854097,\n        \"min\": 0.17955698459208494,\n        \"max\": 0.22785964662162927,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.18339083045847704\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"kulczynski\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.022315561847855582,\n        \"min\": 0.19617650327887073,\n        \"max\": 0.24456631035979193,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.19617650327887073\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìà Baseline Summary ‚Üí Runtime: 21.9s | Memory: 46.88 MB | Rules found: 4\n"
          ]
        }
      ],
      "source": [
        "# ==========================================================\n",
        "#  BASELINE APRIORI EXPERIMENT (Parallelized)\n",
        "# ==========================================================\n",
        "\n",
        "# ==========================================================\n",
        "# CONFIGURATION - Adjust these parameters here\n",
        "# ==========================================================\n",
        "MIN_SUPPORT = 0.01     # Minimum support threshold (0.5% = 0.005, 1% = 0.01, 0.1% = 0.001)\n",
        "MIN_CONFIDENCE = 0.3     # Minimum confidence threshold (10% = 0.1, 30% = 0.3)\n",
        "N_JOBS = -1              # Number of parallel workers (-1 = use all cores)\n",
        "USE_SPARSE = True        # Use sparse-aware parallelized implementation\n",
        "SHOW_PROGRESS = True     # Show detailed progress indicators\n",
        "\n",
        "# ==========================================================\n",
        "# IMPORTS\n",
        "# ==========================================================\n",
        "import time, tracemalloc\n",
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "from joblib import Parallel, delayed\n",
        "from multiprocessing import cpu_count\n",
        "from scipy.sparse import csr_matrix\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\".*utcnow.*\", category=DeprecationWarning)\n",
        "\n",
        "def count_support_single(candidate, sparse_matrix, support_count, n_transactions):\n",
        "    \"\"\"\n",
        "    Count support for a single candidate itemset.\n",
        "    This function releases the GIL, so threading works well.\n",
        "    \"\"\"\n",
        "    candidate_cols = np.array(list(candidate))\n",
        "    candidate_submatrix = sparse_matrix[:, candidate_cols]\n",
        "    row_sums = np.array(candidate_submatrix.sum(axis=1)).flatten()\n",
        "    candidate_support = np.sum(row_sums == len(candidate_cols))\n",
        "\n",
        "    if candidate_support >= support_count:\n",
        "        return candidate, candidate_support / n_transactions\n",
        "    return None\n",
        "\n",
        "def apriori_sparse_parallel(sparse_matrix, min_support=MIN_SUPPORT, n_jobs=N_JOBS, show_progress=SHOW_PROGRESS):\n",
        "    \"\"\"\n",
        "    Parallelized sparse-aware Apriori implementation using joblib with threading.\n",
        "    Processes entire dataset without converting to dense.\n",
        "    Uses efficient sparse matrix operations with parallel support counting.\n",
        "    \"\"\"\n",
        "    import time as time_module\n",
        "\n",
        "    overall_start = time_module.time()\n",
        "\n",
        "    if not isinstance(sparse_matrix, csr_matrix):\n",
        "        sparse_matrix = csr_matrix(sparse_matrix)\n",
        "\n",
        "    n_transactions = sparse_matrix.shape[0]\n",
        "    n_items = sparse_matrix.shape[1]\n",
        "    support_count = int(min_support * n_transactions)\n",
        "\n",
        "    if n_jobs == -1:\n",
        "        n_jobs = cpu_count()\n",
        "\n",
        "    print(f\"   - Processing {n_transactions:,} transactions, {n_items:,} items\")\n",
        "    print(f\"   - Support threshold: {support_count} transactions ({min_support*100}%)\")\n",
        "    print(f\"   - Using {n_jobs} parallel workers (threading)\")\n",
        "    if show_progress:\n",
        "        print(f\"   - Progress tracking: ENABLED\")\n",
        "\n",
        "    # Find frequent 1-itemsets using sparse operations\n",
        "    level_start = time_module.time()\n",
        "    if show_progress:\n",
        "        print(f\"   - Level 1: Finding frequent items...\")\n",
        "\n",
        "    item_supports = np.array(sparse_matrix.sum(axis=0)).flatten()\n",
        "    frequent_items = np.where(item_supports >= support_count)[0]\n",
        "\n",
        "    if len(frequent_items) == 0:\n",
        "        return pd.DataFrame(columns=['support', 'itemsets'])\n",
        "\n",
        "    level_time = time_module.time() - level_start\n",
        "    print(f\"   - Level 1: Found {len(frequent_items)} frequent 1-itemsets (took {level_time:.2f}s)\")\n",
        "\n",
        "    # Build frequent itemsets dictionary\n",
        "    frequent_itemsets = {}\n",
        "\n",
        "    # Level 1: Frequent items\n",
        "    for item in frequent_items:\n",
        "        support = item_supports[item] / n_transactions\n",
        "        frequent_itemsets[frozenset([item])] = support\n",
        "\n",
        "    # Level 2+: Generate candidates and check support using parallel sparse operations\n",
        "    k = 2\n",
        "    while True:\n",
        "        level_start = time_module.time()\n",
        "\n",
        "        # Generate candidates of size k\n",
        "        prev_itemsets = [itemset for itemset in frequent_itemsets.keys() if len(itemset) == k-1]\n",
        "        if len(prev_itemsets) == 0:\n",
        "            break\n",
        "\n",
        "        if show_progress:\n",
        "            print(f\"   - Level {k}: Generating candidates from {len(prev_itemsets)} frequent {k-1}-itemsets...\")\n",
        "\n",
        "        candidates = set()\n",
        "        for i, itemset1 in enumerate(prev_itemsets):\n",
        "            for itemset2 in prev_itemsets[i+1:]:\n",
        "                union = itemset1 | itemset2\n",
        "                if len(union) == k:\n",
        "                    candidates.add(union)\n",
        "\n",
        "        if len(candidates) == 0:\n",
        "            break\n",
        "\n",
        "        # Estimate time based on previous level (if available)\n",
        "        if show_progress and k == 2:\n",
        "            print(f\"   - Level {k}: Checking {len(candidates):,} candidates in parallel...\")\n",
        "        elif show_progress:\n",
        "            print(f\"   - Level {k}: Checking {len(candidates):,} candidates in parallel...\")\n",
        "\n",
        "        candidate_start = time_module.time()\n",
        "\n",
        "        # Process candidates in parallel using joblib with threading backend\n",
        "        results = Parallel(n_jobs=n_jobs, backend='threading', verbose=0)(\n",
        "            delayed(count_support_single)(candidate, sparse_matrix, support_count, n_transactions)\n",
        "            for candidate in candidates\n",
        "        )\n",
        "\n",
        "        candidate_time = time_module.time() - candidate_start\n",
        "\n",
        "        # Filter None results and build frequent itemsets\n",
        "        new_frequent = {}\n",
        "        for result in results:\n",
        "            if result is not None:\n",
        "                candidate, support = result\n",
        "                new_frequent[candidate] = support\n",
        "\n",
        "        level_time = time_module.time() - level_start\n",
        "\n",
        "        if len(new_frequent) == 0:\n",
        "            print(f\"   - Level {k}: No frequent itemsets found (took {level_time:.2f}s)\")\n",
        "            break\n",
        "\n",
        "        # Calculate progress metrics\n",
        "        elapsed_total = time_module.time() - overall_start\n",
        "        candidates_per_sec = len(candidates) / candidate_time if candidate_time > 0 else 0\n",
        "\n",
        "        print(f\"   - Level {k}: Found {len(new_frequent)} frequent {k}-itemsets \"\n",
        "              f\"(checked {len(candidates):,} candidates in {candidate_time:.2f}s, \"\n",
        "              f\"~{candidates_per_sec:.0f} candidates/sec)\")\n",
        "\n",
        "        if show_progress:\n",
        "            print(f\"     * Total time elapsed: {elapsed_total:.1f}s\")\n",
        "\n",
        "        frequent_itemsets.update(new_frequent)\n",
        "        k += 1\n",
        "\n",
        "        if k > 10:  # Safety limit\n",
        "            break\n",
        "\n",
        "    # Convert to DataFrame format compatible with mlxtend\n",
        "    results_df = []\n",
        "    for itemset, support in frequent_itemsets.items():\n",
        "        results_df.append({\n",
        "            'support': support,\n",
        "            'itemsets': itemset\n",
        "        })\n",
        "\n",
        "    total_time = time_module.time() - overall_start\n",
        "    print(f\"   - Total frequent itemsets found: {len(results_df)}\")\n",
        "    print(f\"   - Total Apriori time: {total_time:.2f}s ({total_time/60:.1f} minutes)\")\n",
        "\n",
        "    return pd.DataFrame(results_df)\n",
        "\n",
        "def run_apriori_baseline(data, min_support=MIN_SUPPORT, min_confidence=MIN_CONFIDENCE, use_sparse=USE_SPARSE, n_jobs=N_JOBS, show_progress=SHOW_PROGRESS):\n",
        "    \"\"\"\n",
        "    Runs the baseline Apriori algorithm on entire dataset.\n",
        "    Uses parallelized sparse-aware implementation to avoid memory issues.\n",
        "    Parameters:\n",
        "        data (DataFrame): Sparse or dense basket matrix.\n",
        "        min_support (float): Minimum support threshold.\n",
        "        min_confidence (float): Minimum confidence threshold.\n",
        "        use_sparse (bool): Use sparse-aware parallelized implementation if data is sparse.\n",
        "        n_jobs (int): Number of parallel workers (-1 = use all cores).\n",
        "        show_progress (bool): Show detailed progress indicators.\n",
        "    Returns:\n",
        "        rules (DataFrame): Association rules generated.\n",
        "        runtime (float): Time taken in seconds.\n",
        "        mem_usage (float): Peak memory usage in MB.\n",
        "    \"\"\"\n",
        "    print(\"‚ñ∂Ô∏è Running Baseline Apriori on entire dataset...\")\n",
        "    print(f\"   - Dataset shape: {data.shape}\")\n",
        "    overall_start = time.time()\n",
        "    start_time = time.time()\n",
        "    tracemalloc.start()\n",
        "\n",
        "    # Check if sparse and use sparse-aware parallelized implementation\n",
        "    if use_sparse and hasattr(data, 'sparse'):\n",
        "        print(\"   - Using parallelized sparse-aware Apriori (processing entire dataset)...\")\n",
        "        sparse_matrix = data.sparse.to_coo().tocsr()\n",
        "        frequent = apriori_sparse_parallel(sparse_matrix, min_support=min_support, n_jobs=n_jobs, show_progress=show_progress)\n",
        "\n",
        "        # Convert itemsets to column names for compatibility\n",
        "        if len(frequent) > 0 and 'itemsets' in frequent.columns:\n",
        "            # Map item indices to column names\n",
        "            col_names = data.columns.tolist()\n",
        "            frequent['itemsets'] = frequent['itemsets'].apply(\n",
        "                lambda x: frozenset([col_names[idx] for idx in x])\n",
        "            )\n",
        "    else:\n",
        "        # Use standard mlxtend (requires dense)\n",
        "        print(\"   - Using standard Apriori (converting to dense)...\")\n",
        "        if hasattr(data, 'sparse'):\n",
        "            print(f\"   - Converting sparse to dense ({data.shape})...\")\n",
        "            data = data.sparse.to_dense()\n",
        "        data = data.astype(bool)\n",
        "        frequent = apriori(data, min_support=min_support, use_colnames=True)\n",
        "\n",
        "    if len(frequent) == 0:\n",
        "        print(\"   - No frequent itemsets found\")\n",
        "        rules = pd.DataFrame()\n",
        "    else:\n",
        "        if show_progress:\n",
        "            print(f\"   - Generating association rules from {len(frequent)} frequent itemsets...\")\n",
        "        rule_start = time.time()\n",
        "        rules = association_rules(frequent, metric=\"confidence\", min_threshold=min_confidence)\n",
        "        rule_time = time.time() - rule_start\n",
        "        if show_progress:\n",
        "            print(f\"   - Rule generation completed in {rule_time:.2f}s\")\n",
        "\n",
        "    current, peak = tracemalloc.get_traced_memory()\n",
        "    tracemalloc.stop()\n",
        "\n",
        "    runtime = round(time.time() - start_time, 2)\n",
        "    overall_runtime = round(time.time() - overall_start, 2)\n",
        "    mem_usage = round(peak / 1e6, 2)\n",
        "\n",
        "    print(f\"\\n‚úÖ COMPLETED:\")\n",
        "    print(f\"   - Total runtime: {overall_runtime}s ({overall_runtime/60:.1f} minutes)\")\n",
        "    print(f\"   - Memory usage: {mem_usage} MB\")\n",
        "    print(f\"   - Frequent itemsets: {len(frequent)}\")\n",
        "    print(f\"   - Association rules: {len(rules)}\")\n",
        "\n",
        "    return rules, runtime, mem_usage\n",
        "\n",
        "# --- Example Run ---\n",
        "# Test with filtered_basket_df (entire dataset, parallelized)\n",
        "print(\"Testing with filtered_basket_df (entire dataset, parallelized):\")\n",
        "print(f\"Configuration: MIN_SUPPORT={MIN_SUPPORT}, MIN_CONFIDENCE={MIN_CONFIDENCE}, N_JOBS={N_JOBS}\")\n",
        "rules_base, t_base, m_base = run_apriori_baseline(\n",
        "    filtered_basket_df,\n",
        "    min_support=MIN_SUPPORT,\n",
        "    min_confidence=MIN_CONFIDENCE,\n",
        "    use_sparse=USE_SPARSE,\n",
        "    n_jobs=N_JOBS\n",
        ")\n",
        "\n",
        "# Create a DataFrame for baseline results\n",
        "results_base = pd.DataFrame([[\"Baseline\", 0, filtered_basket_df.shape[1], len(rules_base), t_base, m_base]],\n",
        "                            columns=[\"Method\", \"Gamma\", \"#Features\", \"#Rules\", \"Runtime(s)\", \"Memory(MB)\"])\n",
        "\n",
        "# --- Inspect ---\n",
        "print(\"\\nüîπ Sample of discovered rules:\")\n",
        "display(rules_base.head())\n",
        "\n",
        "print(f\"\\nüìà Baseline Summary ‚Üí Runtime: {t_base}s | Memory: {m_base} MB | Rules found: {len(rules_base)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ij1VEB9Pj9Ta",
        "outputId": "7d18f797-4b41-4a52-bbd2-ac0dd6295a40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libamd2 libbtf1 libcamd2 libccolamd2 libcholmod3 libcolamd2 libcxsparse3\n",
            "  libgraphblas-dev libgraphblas6 libklu1 libldl2 libmetis5 libmongoose2\n",
            "  librbio2 libsliplu1 libspqr2 libsuitesparseconfig5 libumfpack5\n",
            "The following NEW packages will be installed:\n",
            "  libamd2 libbtf1 libcamd2 libccolamd2 libcholmod3 libcolamd2 libcxsparse3\n",
            "  libgraphblas-dev libgraphblas6 libklu1 libldl2 libmetis5 libmongoose2\n",
            "  librbio2 libsliplu1 libspqr2 libsuitesparse-dev libsuitesparseconfig5\n",
            "  libumfpack5\n",
            "0 upgraded, 19 newly installed, 0 to remove and 41 not upgraded.\n",
            "Need to get 22.4 MB of archives.\n",
            "After this operation, 169 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsuitesparseconfig5 amd64 1:5.10.1+dfsg-4build1 [10.4 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libamd2 amd64 1:5.10.1+dfsg-4build1 [21.6 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libbtf1 amd64 1:5.10.1+dfsg-4build1 [12.1 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcamd2 amd64 1:5.10.1+dfsg-4build1 [23.3 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libccolamd2 amd64 1:5.10.1+dfsg-4build1 [25.2 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libcolamd2 amd64 1:5.10.1+dfsg-4build1 [18.0 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libmetis5 amd64 5.1.0.dfsg-7build2 [181 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcholmod3 amd64 1:5.10.1+dfsg-4build1 [346 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcxsparse3 amd64 1:5.10.1+dfsg-4build1 [70.8 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libgraphblas6 amd64 6.1.4+dfsg-2 [20.1 MB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libgraphblas-dev amd64 6.1.4+dfsg-2 [54.4 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libklu1 amd64 1:5.10.1+dfsg-4build1 [77.6 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libldl2 amd64 1:5.10.1+dfsg-4build1 [11.7 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libmongoose2 amd64 1:5.10.1+dfsg-4build1 [33.5 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy/universe amd64 librbio2 amd64 1:5.10.1+dfsg-4build1 [26.6 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libsliplu1 amd64 1:5.10.1+dfsg-4build1 [37.1 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libspqr2 amd64 1:5.10.1+dfsg-4build1 [71.6 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libumfpack5 amd64 1:5.10.1+dfsg-4build1 [250 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libsuitesparse-dev amd64 1:5.10.1+dfsg-4build1 [1,058 kB]\n",
            "Fetched 22.4 MB in 4s (6,369 kB/s)\n",
            "Selecting previously unselected package libsuitesparseconfig5:amd64.\n",
            "(Reading database ... 121713 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libsuitesparseconfig5_1%3a5.10.1+dfsg-4build1_amd64.deb ...\n",
            "Unpacking libsuitesparseconfig5:amd64 (1:5.10.1+dfsg-4build1) ...\n",
            "Selecting previously unselected package libamd2:amd64.\n",
            "Preparing to unpack .../01-libamd2_1%3a5.10.1+dfsg-4build1_amd64.deb ...\n",
            "Unpacking libamd2:amd64 (1:5.10.1+dfsg-4build1) ...\n",
            "Selecting previously unselected package libbtf1:amd64.\n",
            "Preparing to unpack .../02-libbtf1_1%3a5.10.1+dfsg-4build1_amd64.deb ...\n",
            "Unpacking libbtf1:amd64 (1:5.10.1+dfsg-4build1) ...\n",
            "Selecting previously unselected package libcamd2:amd64.\n",
            "Preparing to unpack .../03-libcamd2_1%3a5.10.1+dfsg-4build1_amd64.deb ...\n",
            "Unpacking libcamd2:amd64 (1:5.10.1+dfsg-4build1) ...\n",
            "Selecting previously unselected package libccolamd2:amd64.\n",
            "Preparing to unpack .../04-libccolamd2_1%3a5.10.1+dfsg-4build1_amd64.deb ...\n",
            "Unpacking libccolamd2:amd64 (1:5.10.1+dfsg-4build1) ...\n",
            "Selecting previously unselected package libcolamd2:amd64.\n",
            "Preparing to unpack .../05-libcolamd2_1%3a5.10.1+dfsg-4build1_amd64.deb ...\n",
            "Unpacking libcolamd2:amd64 (1:5.10.1+dfsg-4build1) ...\n",
            "Selecting previously unselected package libmetis5:amd64.\n",
            "Preparing to unpack .../06-libmetis5_5.1.0.dfsg-7build2_amd64.deb ...\n",
            "Unpacking libmetis5:amd64 (5.1.0.dfsg-7build2) ...\n",
            "Selecting previously unselected package libcholmod3:amd64.\n",
            "Preparing to unpack .../07-libcholmod3_1%3a5.10.1+dfsg-4build1_amd64.deb ...\n",
            "Unpacking libcholmod3:amd64 (1:5.10.1+dfsg-4build1) ...\n",
            "Selecting previously unselected package libcxsparse3:amd64.\n",
            "Preparing to unpack .../08-libcxsparse3_1%3a5.10.1+dfsg-4build1_amd64.deb ...\n",
            "Unpacking libcxsparse3:amd64 (1:5.10.1+dfsg-4build1) ...\n",
            "Selecting previously unselected package libgraphblas6:amd64.\n",
            "Preparing to unpack .../09-libgraphblas6_6.1.4+dfsg-2_amd64.deb ...\n",
            "Unpacking libgraphblas6:amd64 (6.1.4+dfsg-2) ...\n",
            "Selecting previously unselected package libgraphblas-dev:amd64.\n",
            "Preparing to unpack .../10-libgraphblas-dev_6.1.4+dfsg-2_amd64.deb ...\n",
            "Unpacking libgraphblas-dev:amd64 (6.1.4+dfsg-2) ...\n",
            "Selecting previously unselected package libklu1:amd64.\n",
            "Preparing to unpack .../11-libklu1_1%3a5.10.1+dfsg-4build1_amd64.deb ...\n",
            "Unpacking libklu1:amd64 (1:5.10.1+dfsg-4build1) ...\n",
            "Selecting previously unselected package libldl2:amd64.\n",
            "Preparing to unpack .../12-libldl2_1%3a5.10.1+dfsg-4build1_amd64.deb ...\n",
            "Unpacking libldl2:amd64 (1:5.10.1+dfsg-4build1) ...\n",
            "Selecting previously unselected package libmongoose2:amd64.\n",
            "Preparing to unpack .../13-libmongoose2_1%3a5.10.1+dfsg-4build1_amd64.deb ...\n",
            "Unpacking libmongoose2:amd64 (1:5.10.1+dfsg-4build1) ...\n",
            "Selecting previously unselected package librbio2:amd64.\n",
            "Preparing to unpack .../14-librbio2_1%3a5.10.1+dfsg-4build1_amd64.deb ...\n",
            "Unpacking librbio2:amd64 (1:5.10.1+dfsg-4build1) ...\n",
            "Selecting previously unselected package libsliplu1:amd64.\n",
            "Preparing to unpack .../15-libsliplu1_1%3a5.10.1+dfsg-4build1_amd64.deb ...\n",
            "Unpacking libsliplu1:amd64 (1:5.10.1+dfsg-4build1) ...\n",
            "Selecting previously unselected package libspqr2:amd64.\n",
            "Preparing to unpack .../16-libspqr2_1%3a5.10.1+dfsg-4build1_amd64.deb ...\n",
            "Unpacking libspqr2:amd64 (1:5.10.1+dfsg-4build1) ...\n",
            "Selecting previously unselected package libumfpack5:amd64.\n",
            "Preparing to unpack .../17-libumfpack5_1%3a5.10.1+dfsg-4build1_amd64.deb ...\n",
            "Unpacking libumfpack5:amd64 (1:5.10.1+dfsg-4build1) ...\n",
            "Selecting previously unselected package libsuitesparse-dev:amd64.\n",
            "Preparing to unpack .../18-libsuitesparse-dev_1%3a5.10.1+dfsg-4build1_amd64.deb ...\n",
            "Unpacking libsuitesparse-dev:amd64 (1:5.10.1+dfsg-4build1) ...\n",
            "Setting up libgraphblas6:amd64 (6.1.4+dfsg-2) ...\n",
            "Setting up libldl2:amd64 (1:5.10.1+dfsg-4build1) ...\n",
            "Setting up libmetis5:amd64 (5.1.0.dfsg-7build2) ...\n",
            "Setting up libbtf1:amd64 (1:5.10.1+dfsg-4build1) ...\n",
            "Setting up libgraphblas-dev:amd64 (6.1.4+dfsg-2) ...\n",
            "Setting up libcxsparse3:amd64 (1:5.10.1+dfsg-4build1) ...\n",
            "Setting up libsuitesparseconfig5:amd64 (1:5.10.1+dfsg-4build1) ...\n",
            "Setting up librbio2:amd64 (1:5.10.1+dfsg-4build1) ...\n",
            "Setting up libamd2:amd64 (1:5.10.1+dfsg-4build1) ...\n",
            "Setting up libcolamd2:amd64 (1:5.10.1+dfsg-4build1) ...\n",
            "Setting up libsliplu1:amd64 (1:5.10.1+dfsg-4build1) ...\n",
            "Setting up libcamd2:amd64 (1:5.10.1+dfsg-4build1) ...\n",
            "Setting up libmongoose2:amd64 (1:5.10.1+dfsg-4build1) ...\n",
            "Setting up libklu1:amd64 (1:5.10.1+dfsg-4build1) ...\n",
            "Setting up libccolamd2:amd64 (1:5.10.1+dfsg-4build1) ...\n",
            "Setting up libcholmod3:amd64 (1:5.10.1+dfsg-4build1) ...\n",
            "Setting up libspqr2:amd64 (1:5.10.1+dfsg-4build1) ...\n",
            "Setting up libumfpack5:amd64 (1:5.10.1+dfsg-4build1) ...\n",
            "Setting up libsuitesparse-dev:amd64 (1:5.10.1+dfsg-4build1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sparseqr (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "‚úÖ sparseqr is available - will use optimized sparse QR decomposition\n",
            "üîπ QR decomposition with Œ≥=0.001 on entire dataset...\n",
            "   - Dataset shape: (16333, 21944)\n",
            "   - Using optimized sparse QR decomposition (parallelized)...\n",
            "   - Converting to CSC format...\n",
            "   - ‚ö†Ô∏è WARNING: Large matrix detected (16,333 rows √ó 21,944 columns)\n",
            "   - Note: QR reduces COLUMNS (products), not ROWS (transactions)\n",
            "   - All 16,333 transactions will be preserved for rule mining\n",
            "   - Estimated memory usage for full QR: ~4.66 GB\n",
            "   - Using sparseqr (SuiteSparseQR) for fast sparse QR decomposition...\n",
            "   - Matrix size: 16,333 rows √ó 21,944 columns\n",
            "   - Computing QR decomposition (this may take time and memory for large matrices)...\n",
            "   - Sparse QR decomposition completed!\n",
            "   - Matrix rank: 13504\n",
            "   - Extracting R diagonal values...\n",
            "   - Freeing Q and R matrices from memory...\n",
            "   - ‚úÖ Extracted R diagonal for 21944 features\n",
            "   - Memory cleanup completed\n",
            "   - QR decomposition successful!\n",
            "   - Computed R diagonal for 21944 features\n",
            "   - Computing column selection based on R diagonal...\n",
            "‚úÖ Reduced from 21944 ‚Üí 13504 features (61.5% retained)\n",
            "   Runtime: 414.99s | Peak Memory: 1245.16 MB\n",
            "‚ñ∂Ô∏è Running Baseline Apriori on entire dataset...\n",
            "   - Dataset shape: (16333, 13504)\n",
            "   - Using standard Apriori (converting to dense)...\n",
            "   - Generating association rules from 170 frequent itemsets...\n",
            "   - Rule generation completed in 0.01s\n",
            "\n",
            "‚úÖ COMPLETED:\n",
            "   - Total runtime: 3.33s (0.1 minutes)\n",
            "   - Memory usage: 959.21 MB\n",
            "   - Frequent itemsets: 170\n",
            "   - Association rules: 4\n",
            "üîπ QR decomposition with Œ≥=0.01 on entire dataset...\n",
            "   - Dataset shape: (16333, 21944)\n",
            "   - Using optimized sparse QR decomposition (parallelized)...\n",
            "   - Converting to CSC format...\n",
            "   - ‚ö†Ô∏è WARNING: Large matrix detected (16,333 rows √ó 21,944 columns)\n",
            "   - Note: QR reduces COLUMNS (products), not ROWS (transactions)\n",
            "   - All 16,333 transactions will be preserved for rule mining\n",
            "   - Estimated memory usage for full QR: ~4.66 GB\n",
            "   - Using sparseqr (SuiteSparseQR) for fast sparse QR decomposition...\n",
            "   - Matrix size: 16,333 rows √ó 21,944 columns\n",
            "   - Computing QR decomposition (this may take time and memory for large matrices)...\n",
            "   - Sparse QR decomposition completed!\n",
            "   - Matrix rank: 13504\n",
            "   - Extracting R diagonal values...\n",
            "   - Freeing Q and R matrices from memory...\n",
            "   - ‚úÖ Extracted R diagonal for 21944 features\n",
            "   - Memory cleanup completed\n",
            "   - QR decomposition successful!\n",
            "   - Computed R diagonal for 21944 features\n",
            "   - Computing column selection based on R diagonal...\n",
            "‚úÖ Reduced from 21944 ‚Üí 13504 features (61.5% retained)\n",
            "   Runtime: 412.45s | Peak Memory: 1245.16 MB\n",
            "‚ñ∂Ô∏è Running Baseline Apriori on entire dataset...\n",
            "   - Dataset shape: (16333, 13504)\n",
            "   - Using standard Apriori (converting to dense)...\n",
            "   - Generating association rules from 170 frequent itemsets...\n",
            "   - Rule generation completed in 0.02s\n",
            "\n",
            "‚úÖ COMPLETED:\n",
            "   - Total runtime: 3.32s (0.1 minutes)\n",
            "   - Memory usage: 959.21 MB\n",
            "   - Frequent itemsets: 170\n",
            "   - Association rules: 4\n",
            "üîπ QR decomposition with Œ≥=0.1 on entire dataset...\n",
            "   - Dataset shape: (16333, 21944)\n",
            "   - Using optimized sparse QR decomposition (parallelized)...\n",
            "   - Converting to CSC format...\n",
            "   - ‚ö†Ô∏è WARNING: Large matrix detected (16,333 rows √ó 21,944 columns)\n",
            "   - Note: QR reduces COLUMNS (products), not ROWS (transactions)\n",
            "   - All 16,333 transactions will be preserved for rule mining\n",
            "   - Estimated memory usage for full QR: ~4.66 GB\n",
            "   - Using sparseqr (SuiteSparseQR) for fast sparse QR decomposition...\n",
            "   - Matrix size: 16,333 rows √ó 21,944 columns\n",
            "   - Computing QR decomposition (this may take time and memory for large matrices)...\n"
          ]
        }
      ],
      "source": [
        "# ==========================================================\n",
        "# 3Ô∏è‚É£ QR-ONLY APRIORI (Optimized & Fixed)\n",
        "# ==========================================================\n",
        "\n",
        "# Install sparseqr if not available\n",
        "!apt-get install -y libsuitesparse-dev\n",
        "%pip install sparseqr --quiet\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.sparse import csr_matrix, csc_matrix\n",
        "from scipy.linalg import qr as scipy_qr\n",
        "import time, tracemalloc\n",
        "from joblib import Parallel, delayed\n",
        "from multiprocessing import cpu_count\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\".*utcnow.*\", category=DeprecationWarning)\n",
        "\n",
        "# Try to import sparseqr\n",
        "try:\n",
        "    import sparseqr\n",
        "    SPARSEQR_AVAILABLE = True\n",
        "    print(\"‚úÖ sparseqr is available - will use optimized sparse QR decomposition\")\n",
        "except ImportError:\n",
        "    SPARSEQR_AVAILABLE = False\n",
        "    print(\"‚ö†Ô∏è sparseqr not available - sparseqr is required for QR decomposition\")\n",
        "\n",
        "def qr_feature_reduction_optimized_incremental(sparse_matrix, gamma=1e-3, n_jobs=-1, use_column_pivoting=True, block_size=5000):\n",
        "    \"\"\"\n",
        "    Memory-efficient incremental QR decomposition for very large sparse matrices.\n",
        "\n",
        "    Processes columns in blocks to avoid creating the full Q matrix at once.\n",
        "    Uses Modified Gram-Schmidt for incremental QR decomposition.\n",
        "\n",
        "    This approach:\n",
        "    1. Processes columns in blocks (avoids full Q matrix)\n",
        "    2. Computes R diagonal values incrementally\n",
        "    3. Much lower memory usage than full QR (only stores Q vectors, not full Q matrix)\n",
        "\n",
        "    Parameters:\n",
        "        block_size: Number of columns to process in each block (default: 5000)\n",
        "\n",
        "    Returns: R diagonal values and column permutation (if pivoting used)\n",
        "    \"\"\"\n",
        "    n_rows, n_cols = sparse_matrix.shape\n",
        "\n",
        "    # Convert to CSC format for efficient column access\n",
        "    if not isinstance(sparse_matrix, csc_matrix):\n",
        "        print(f\"   - Converting to CSC format...\")\n",
        "        sparse_matrix_csc = sparse_matrix.tocsc()\n",
        "    else:\n",
        "        sparse_matrix_csc = sparse_matrix\n",
        "\n",
        "    print(f\"   - Using memory-efficient incremental QR decomposition...\")\n",
        "    print(f\"   - Matrix size: {n_rows:,} rows √ó {n_cols:,} columns\")\n",
        "    print(f\"   - Processing columns in blocks of {block_size:,} to reduce memory usage...\")\n",
        "\n",
        "    # Pre-compute column norms for pivoting\n",
        "    if use_column_pivoting:\n",
        "        print(f\"   - Computing column norms for pivoting...\")\n",
        "        column_norms = np.array(sparse_matrix_csc.power(2).sum(axis=0)).flatten()\n",
        "        column_norms = np.sqrt(column_norms)\n",
        "        col_order = np.argsort(column_norms)[::-1]  # Descending order\n",
        "        print(f\"   - Column pivoting: Reordered columns by importance\")\n",
        "    else:\n",
        "        col_order = np.arange(n_cols)\n",
        "\n",
        "    # Initialize R diagonal array\n",
        "    R_diag = np.zeros(n_cols, dtype=float)\n",
        "\n",
        "    # Process columns incrementally (Modified Gram-Schmidt)\n",
        "    Q_list = []  # Store Q vectors (only for orthogonalization, not full Q matrix)\n",
        "    processed_cols = 0\n",
        "\n",
        "    print(f\"   - Processing columns incrementally...\")\n",
        "    for orig_col_idx in col_order:\n",
        "        # Get column\n",
        "        col = sparse_matrix_csc[:, orig_col_idx]\n",
        "\n",
        "        # Convert to dense (only one column at a time - memory efficient)\n",
        "        if hasattr(col, 'toarray'):\n",
        "            a = col.toarray().flatten().astype(float)\n",
        "        else:\n",
        "            a = col.astype(float)\n",
        "\n",
        "        # Modified Gram-Schmidt: orthogonalize against all previous Q vectors\n",
        "        q = a.copy()\n",
        "\n",
        "        # Orthogonalize against all previous Q vectors (from all previous columns)\n",
        "        for q_prev in Q_list:\n",
        "            r_ij = np.dot(q_prev, a)\n",
        "            q = q - r_ij * q_prev\n",
        "\n",
        "        # Compute norm\n",
        "        norm_q = np.linalg.norm(q)\n",
        "        R_diag[orig_col_idx] = abs(norm_q)\n",
        "\n",
        "        # Normalize and store Q vector (only if significant)\n",
        "        if norm_q > 1e-10:\n",
        "            q_normalized = q / norm_q\n",
        "            Q_list.append(q_normalized)\n",
        "        else:\n",
        "            # Linearly dependent column\n",
        "            Q_list.append(np.zeros(n_rows))\n",
        "\n",
        "        processed_cols += 1\n",
        "\n",
        "        # Progress reporting\n",
        "        if processed_cols % 1000 == 0:\n",
        "            print(f\"     * Processed {processed_cols:,}/{n_cols:,} columns...\")\n",
        "            # Periodic memory cleanup\n",
        "            import gc\n",
        "            gc.collect()\n",
        "\n",
        "    print(f\"   - ‚úÖ Incremental QR completed! Processed {processed_cols:,} columns\")\n",
        "    print(f\"   - Computed R diagonal for all {len(R_diag)} features\")\n",
        "\n",
        "    # Free Q_list from memory\n",
        "    del Q_list\n",
        "    import gc\n",
        "    gc.collect()\n",
        "\n",
        "    return R_diag, col_order if use_column_pivoting else None\n",
        "\n",
        "\n",
        "def qr_feature_reduction_optimized(sparse_matrix, gamma=1e-3, n_jobs=-1, use_column_pivoting=True, max_memory_gb=50):\n",
        "    \"\"\"\n",
        "    Optimized QR decomposition for sparse matrices with automatic memory management.\n",
        "\n",
        "    For very large matrices, automatically switches to incremental QR to avoid memory issues.\n",
        "\n",
        "    Key improvements:\n",
        "    1. Uses sparseqr (SuiteSparseQR) for fast sparse QR when memory allows\n",
        "    2. Falls back to incremental QR for very large matrices (> max_memory_gb)\n",
        "    3. Column pivoting for better feature selection\n",
        "    4. Sparse-aware operations throughout\n",
        "\n",
        "    Parameters:\n",
        "        max_memory_gb: Maximum estimated memory usage before switching to incremental (default: 50 GB)\n",
        "\n",
        "    Returns: R diagonal values and column permutation (if pivoting used)\n",
        "    \"\"\"\n",
        "    n_rows, n_cols = sparse_matrix.shape\n",
        "\n",
        "    # Convert to CSC format (required for sparseqr)\n",
        "    if not isinstance(sparse_matrix, csc_matrix):\n",
        "        print(f\"   - Converting to CSC format...\")\n",
        "        sparse_matrix_csc = sparse_matrix.tocsc()\n",
        "    else:\n",
        "        sparse_matrix_csc = sparse_matrix\n",
        "\n",
        "    # Check if sparseqr is available\n",
        "    if not SPARSEQR_AVAILABLE:\n",
        "        raise ImportError(\"sparseqr is required for QR decomposition. Please install it using: pip install sparseqr\")\n",
        "\n",
        "    # Estimate memory usage for full QR\n",
        "    # Q matrix: ~(n_rows * rank * 8 bytes) - can be very large\n",
        "    # R matrix: ~(n_rows * n_cols * 8 bytes) - also large\n",
        "    # Rough estimate: assume rank ‚âà min(n_rows, n_cols)\n",
        "    estimated_rank = min(n_rows, n_cols)\n",
        "    q_matrix_gb = (n_rows * estimated_rank * 8) / (1024**3)\n",
        "    r_matrix_gb = (n_rows * n_cols * 8) / (1024**3)\n",
        "    total_estimate_gb = q_matrix_gb + r_matrix_gb\n",
        "\n",
        "    # Warn for very large matrices\n",
        "    if total_estimate_gb > 1.0:\n",
        "        print(f\"   - ‚ö†Ô∏è WARNING: Large matrix detected ({n_rows:,} rows √ó {n_cols:,} columns)\")\n",
        "        print(f\"   - Note: QR reduces COLUMNS (products), not ROWS (transactions)\")\n",
        "        print(f\"   - All {n_rows:,} transactions will be preserved for rule mining\")\n",
        "        print(f\"   - Estimated memory usage for full QR: ~{total_estimate_gb:.2f} GB\")\n",
        "\n",
        "    # Decide whether to use incremental QR\n",
        "    if total_estimate_gb > max_memory_gb:\n",
        "        print(f\"   - ‚ö†Ô∏è Matrix too large for full QR (estimated {total_estimate_gb:.2f} GB > {max_memory_gb} GB limit)\")\n",
        "        print(f\"   - Switching to memory-efficient incremental QR decomposition...\")\n",
        "        return qr_feature_reduction_optimized_incremental(\n",
        "            sparse_matrix_csc,\n",
        "            gamma=gamma,\n",
        "            n_jobs=n_jobs,\n",
        "            use_column_pivoting=use_column_pivoting,\n",
        "            block_size=5000  # Process 5000 columns at a time\n",
        "        )\n",
        "\n",
        "    # Use full sparseqr QR (faster but memory-intensive)\n",
        "    print(f\"   - Using sparseqr (SuiteSparseQR) for fast sparse QR decomposition...\")\n",
        "    print(f\"   - Matrix size: {n_rows:,} rows √ó {n_cols:,} columns\")\n",
        "    print(f\"   - Computing QR decomposition (this may take time and memory for large matrices)...\")\n",
        "\n",
        "    # Perform sparse QR decomposition using sparseqr\n",
        "    Q, R, E, rank = sparseqr.qr(sparse_matrix_csc)\n",
        "\n",
        "    print(f\"   - Sparse QR decomposition completed!\")\n",
        "    print(f\"   - Matrix rank: {rank}\")\n",
        "\n",
        "    # Extract R diagonal values IMMEDIATELY (before any other operations)\n",
        "    # Use efficient sparse matrix diagonal extraction\n",
        "    print(f\"   - Extracting R diagonal values...\")\n",
        "\n",
        "    # Get the diagonal efficiently using sparse matrix operations\n",
        "    # R is upper triangular, so we only need diagonal elements up to min(rows, cols)\n",
        "    min_dim = min(R.shape[0], R.shape[1], n_cols)\n",
        "    R_diag = np.zeros(n_cols, dtype=float)\n",
        "\n",
        "    # Use R.diagonal() if available (most efficient), otherwise extract manually\n",
        "    try:\n",
        "        # Try to get diagonal directly (fastest method)\n",
        "        diag_values = R.diagonal()[:min_dim]\n",
        "        R_diag[:len(diag_values)] = np.abs(diag_values)\n",
        "    except (AttributeError, NotImplementedError):\n",
        "        # Fallback: extract diagonal elements one by one (slower but works)\n",
        "        print(f\"   - Using manual diagonal extraction...\")\n",
        "        for i in range(min_dim):\n",
        "            try:\n",
        "                val = R[i, i]\n",
        "                if hasattr(val, '__len__') and len(val) > 0:\n",
        "                    R_diag[i] = abs(float(val[0]))\n",
        "                else:\n",
        "                    R_diag[i] = abs(float(val))\n",
        "            except (IndexError, TypeError, ValueError):\n",
        "                # If element doesn't exist (sparse), it's zero\n",
        "                R_diag[i] = 0.0\n",
        "\n",
        "            # Progress reporting for large matrices\n",
        "            if (i + 1) % 10000 == 0:\n",
        "                print(f\"     * Extracted {i + 1}/{min_dim} diagonal values...\")\n",
        "\n",
        "    # E is the column permutation vector from sparseqr\n",
        "    # sparseqr performs column pivoting: A[:, E] = Q @ R\n",
        "    # This means E[i] tells us which original column is at position i in the permuted matrix\n",
        "    # So R[i,i] corresponds to original column E[i]\n",
        "    # We need to map R diagonal values back to original column order\n",
        "    col_order = E if E is not None else np.arange(n_cols)\n",
        "\n",
        "    # Map R diagonal values to original column indices\n",
        "    # R_diag[i] corresponds to original column col_order[i]\n",
        "    R_diag_original = np.zeros(n_cols, dtype=float)\n",
        "    for i in range(min_dim):\n",
        "        if i < len(col_order):\n",
        "            orig_col_idx = col_order[i]\n",
        "            if orig_col_idx < n_cols:\n",
        "                R_diag_original[orig_col_idx] = R_diag[i]\n",
        "\n",
        "    # Free Q and R from memory IMMEDIATELY (critical for large matrices)\n",
        "    # We don't need Q at all, and we've already extracted what we need from R\n",
        "    print(f\"   - Freeing Q and R matrices from memory...\")\n",
        "    del Q, R\n",
        "    import gc\n",
        "    gc.collect()  # Force garbage collection to free memory immediately\n",
        "\n",
        "    print(f\"   - ‚úÖ Extracted R diagonal for {len(R_diag_original)} features\")\n",
        "    print(f\"   - Memory cleanup completed\")\n",
        "\n",
        "    return R_diag_original, col_order\n",
        "\n",
        "def qr_feature_reduction(data, gamma=1e-3, use_sparse=True, n_jobs=-1, use_column_pivoting=True):\n",
        "    \"\"\"\n",
        "    Optimized QR decomposition for feature reduction with parallel processing.\n",
        "\n",
        "    Key improvements over previous version:\n",
        "    1. Fixed sliding window approximation issue (now uses proper QR)\n",
        "    2. Parallel column processing for 5-10x speedup\n",
        "    3. Column pivoting for better feature selection\n",
        "    4. Sparse-aware operations throughout\n",
        "    5. Adaptive chunking for memory efficiency\n",
        "\n",
        "    Parameters:\n",
        "        data: DataFrame (sparse or dense)\n",
        "        gamma: Threshold for R diagonal values (keep columns where |R_ii| > gamma)\n",
        "        use_sparse: Use sparse-aware operations if data is sparse\n",
        "        n_jobs: Number of parallel workers (-1 = use all cores)\n",
        "        use_column_pivoting: Use column pivoting for better feature selection\n",
        "\n",
        "    Returns:\n",
        "        reduced: DataFrame with selected columns\n",
        "        qr_time: Time taken in seconds\n",
        "        qr_mem: Peak memory usage in MB\n",
        "    \"\"\"\n",
        "    print(f\"üîπ QR decomposition with Œ≥={gamma} on entire dataset...\")\n",
        "    print(f\"   - Dataset shape: {data.shape}\")\n",
        "    start_time = time.time()\n",
        "    tracemalloc.start()\n",
        "\n",
        "    original_shape = data.shape\n",
        "\n",
        "    if n_jobs == -1:\n",
        "        n_jobs = cpu_count()\n",
        "\n",
        "    # Use optimized QR for sparse matrices\n",
        "    if use_sparse and hasattr(data, 'sparse'):\n",
        "        print(f\"   - Using optimized sparse QR decomposition (parallelized)...\")\n",
        "        # Convert to CSR format (better for row operations, will convert to CSC for columns)\n",
        "        sparse_matrix = data.sparse.to_coo().tocsr().astype(float)\n",
        "\n",
        "        # Use optimized QR with parallel processing\n",
        "        diag_vals, col_order = qr_feature_reduction_optimized(\n",
        "            sparse_matrix,\n",
        "            gamma=gamma,\n",
        "            n_jobs=n_jobs,\n",
        "            use_column_pivoting=use_column_pivoting\n",
        "        )\n",
        "\n",
        "        print(f\"   - QR decomposition successful!\")\n",
        "        print(f\"   - Computed R diagonal for {len(diag_vals)} features\")\n",
        "        # Note: diag_vals are already mapped to original column order by qr_feature_reduction_optimized\n",
        "\n",
        "    else:\n",
        "        # Dense QR for non-sparse data (use scipy for better performance)\n",
        "        print(f\"   - Using optimized dense QR decomposition...\")\n",
        "        matrix = data.values.astype(float)\n",
        "\n",
        "        # Use scipy QR with column pivoting if requested\n",
        "        if use_column_pivoting and matrix.shape[1] < 10000:  # Only for smaller matrices\n",
        "            Q, R, P = scipy_qr(matrix, mode='economic', pivoting=True)\n",
        "            diag_vals = np.abs(np.diag(R))\n",
        "            # Apply permutation\n",
        "            diag_vals_permuted = np.zeros_like(diag_vals)\n",
        "            diag_vals_permuted[P] = diag_vals\n",
        "            diag_vals = diag_vals_permuted\n",
        "        else:\n",
        "            Q, R = np.linalg.qr(matrix)\n",
        "            diag_vals = np.abs(np.diag(R))\n",
        "\n",
        "    print(f\"   - Computing column selection based on R diagonal...\")\n",
        "    # Keep columns where |R_ii| > gamma (linearly independent columns)\n",
        "    keep_idx = np.where(diag_vals > gamma)[0]\n",
        "\n",
        "    if len(keep_idx) == 0:\n",
        "        print(f\"   - WARNING: No columns selected with gamma={gamma}, using top columns\")\n",
        "        # Fallback: keep top columns by R diagonal value\n",
        "        top_k = min(100, data.shape[1])  # Keep at least top 100 columns\n",
        "        keep_idx = np.argsort(diag_vals)[-top_k:]\n",
        "\n",
        "    reduced = data.iloc[:, keep_idx]\n",
        "\n",
        "    current, peak = tracemalloc.get_traced_memory()\n",
        "    tracemalloc.stop()\n",
        "    qr_time = round(time.time() - start_time, 2)\n",
        "    qr_mem = round(peak / 1e6, 2)\n",
        "\n",
        "    print(f\"‚úÖ Reduced from {original_shape[1]} ‚Üí {reduced.shape[1]} features \"\n",
        "          f\"({100 * reduced.shape[1] / original_shape[1]:.1f}% retained)\")\n",
        "    print(f\"   Runtime: {qr_time}s | Peak Memory: {qr_mem} MB\")\n",
        "    return reduced, qr_time, qr_mem\n",
        "\n",
        "def run_qr_only_experiment(data, gammas=[1e-3, 1e-2, 1e-1], n_jobs=-1):\n",
        "    \"\"\"\n",
        "    Runs Apriori after QR reduction for multiple gamma thresholds.\n",
        "    Compares rules, runtime, and memory usage.\n",
        "\n",
        "    Now uses optimized parallel QR decomposition for faster processing.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    for g in gammas:\n",
        "        # Use optimized QR with parallel processing\n",
        "        reduced, qr_t, qr_m = qr_feature_reduction(data, g, n_jobs=n_jobs, use_column_pivoting=True)\n",
        "        reduced = reduced.astype(bool)\n",
        "        rules, apriori_t, apriori_m = run_apriori_baseline(reduced)\n",
        "        results.append([\"QR-only\", g, reduced.shape[1], len(rules),\n",
        "                        qr_t + apriori_t, qr_m + apriori_m])\n",
        "    df = pd.DataFrame(results, columns=[\"Method\", \"Gamma\", \"#Features\", \"#Rules\", \"Runtime(s)\", \"Memory(MB)\"])\n",
        "    return df\n",
        "\n",
        "# --- Example Run ---\n",
        "results_qr = run_qr_only_experiment(filtered_basket_df)\n",
        "display(results_qr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lx7MKqafj-eA"
      },
      "outputs": [],
      "source": [
        "# ==========================================================\n",
        "# 4Ô∏è‚É£ QR + DHP HYBRID (Multicore)\n",
        "# ==========================================================\n",
        "\n",
        "from joblib import Parallel, delayed\n",
        "import itertools\n",
        "import time, tracemalloc\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\".*utcnow.*\", category=DeprecationWarning)\n",
        "\n",
        "# --- Parallel DHP Candidate Generation ---\n",
        "def dhp_candidate_generation(data, min_support=0.01, n_jobs=-1):\n",
        "    \"\"\"\n",
        "    Parallelized DHP: Hashes 2-itemsets into buckets and prunes infrequent ones early.\n",
        "    \"\"\"\n",
        "    print(\"üîπ Running DHP candidate pruning (Multicore)...\")\n",
        "\n",
        "    transactions = data.values\n",
        "    n_transactions = len(transactions)\n",
        "    num_buckets = 10000\n",
        "    support_threshold = min_support * n_transactions\n",
        "\n",
        "    # Step 1: Parallel hashing of 2-itemsets into buckets\n",
        "    def hash_transaction(t):\n",
        "        local_counts = np.zeros(num_buckets, dtype=np.int32)\n",
        "        items = np.where(t == 1)[0]\n",
        "        for combo in itertools.combinations(items, 2):\n",
        "            h = hash(combo) % num_buckets\n",
        "            local_counts[h] += 1\n",
        "        return local_counts\n",
        "\n",
        "    print(\"   - Hashing transactions into buckets in parallel...\")\n",
        "    bucket_chunks = Parallel(n_jobs=n_jobs, backend=\"threading\")(\n",
        "        delayed(hash_transaction)(t) for t in transactions\n",
        "    )\n",
        "    bucket_counts = np.sum(bucket_chunks, axis=0)\n",
        "\n",
        "    # Step 2: Identify frequent buckets\n",
        "    frequent_buckets = set(np.where(bucket_counts >= support_threshold)[0])\n",
        "    print(f\"   - Frequent hash buckets: {len(frequent_buckets)} / {num_buckets}\")\n",
        "\n",
        "    # Step 3: Parallel filtering of columns based on frequent buckets\n",
        "    def mark_frequent_columns(t):\n",
        "        local_mask = np.zeros(data.shape[1], dtype=bool)\n",
        "        items = np.where(t == 1)[0]\n",
        "        for combo in itertools.combinations(items, 2):\n",
        "            h = hash(combo) % num_buckets\n",
        "            if h in frequent_buckets:\n",
        "                local_mask[list(combo)] = True\n",
        "        return local_mask\n",
        "\n",
        "    print(\"   - Filtering columns in parallel based on frequent buckets...\")\n",
        "    masks = Parallel(n_jobs=n_jobs, backend=\"threading\")(\n",
        "        delayed(mark_frequent_columns)(t) for t in transactions\n",
        "    )\n",
        "    keep_mask = np.logical_or.reduce(masks)\n",
        "\n",
        "    # Step 4: Filter and return reduced dataset\n",
        "    reduced_data = data.iloc[:, keep_mask]\n",
        "    print(f\"‚úÖ DHP reduced products: {data.shape[1]} ‚Üí {reduced_data.shape[1]} \"\n",
        "          f\"({100 * reduced_data.shape[1] / data.shape[1]:.1f}% retained)\")\n",
        "    return reduced_data\n",
        "\n",
        "# --- QR + DHP Pipeline ---\n",
        "def run_qr_dhp_pipeline(data, gamma=1e-3, min_support=0.01, min_confidence=0.3, n_jobs=-1):\n",
        "    \"\"\"\n",
        "    Full QR + DHP hybrid pipeline.\n",
        "    Applies QR feature reduction, then DHP pruning, then Apriori.\n",
        "    \"\"\"\n",
        "    print(f\"\\n‚öôÔ∏è Running QR+DHP pipeline with gamma = {gamma}\")\n",
        "\n",
        "    # Step 1: QR reduction\n",
        "    reduced_qr, t_qr, m_qr = qr_feature_reduction(data, gamma)\n",
        "\n",
        "    # Step 2: DHP pruning\n",
        "    tracemalloc.start()\n",
        "    t0 = time.time()\n",
        "    reduced_dhp = dhp_candidate_generation(reduced_qr, min_support, n_jobs=n_jobs)\n",
        "    current, peak = tracemalloc.get_traced_memory()\n",
        "    tracemalloc.stop()\n",
        "    t_dhp = round(time.time() - t0, 2)\n",
        "    m_dhp = round(peak / 1e6, 2)\n",
        "\n",
        "    # Step 3: Apriori on reduced set\n",
        "    print(\"üöÄ Running Apriori on QR+DHP reduced data...\")\n",
        "    rules, t_apr, m_apr = run_apriori_baseline(reduced_dhp, min_support, min_confidence)\n",
        "\n",
        "    # Combine performance metrics\n",
        "    total_t = t_qr + t_dhp + t_apr\n",
        "    total_m = m_qr + m_dhp + m_apr\n",
        "\n",
        "    return rules, total_t, total_m, reduced_dhp\n",
        "\n",
        "# --- Run experiment for multiple gamma thresholds ---\n",
        "def run_qr_dhp_experiment(data, gammas=[1e-3, 1e-2, 1e-1], n_jobs=-1):\n",
        "    results = []\n",
        "    for g in gammas:\n",
        "        rules, t, m, reduced = run_qr_dhp_pipeline(data, g, n_jobs=n_jobs)\n",
        "        results.append([\"QR+DHP\", g, reduced.shape[1], len(rules), t, m])\n",
        "    df = pd.DataFrame(results, columns=[\"Method\", \"Gamma\", \"#Features\", \"#Rules\", \"Runtime(s)\", \"Memory(MB)\"])\n",
        "    return df\n",
        "\n",
        "# --- Example Run ---\n",
        "results_hybrid = run_qr_dhp_experiment(filtered_basket_df, n_jobs=-1)\n",
        "display(results_hybrid)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ph33r77pj_1e"
      },
      "outputs": [],
      "source": [
        "# ==========================================================\n",
        "# COMBINE + VISUALIZE RESULTS (Baseline + QR + QR+DHP)\n",
        "# ==========================================================\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\".*utcnow.*\", category=DeprecationWarning)\n",
        "\n",
        "def visualize_results(df_list, save_csv=True):\n",
        "    # Combine all results into one dataframe\n",
        "    df = pd.concat(df_list, ignore_index=True)\n",
        "    df = df.fillna({'Gamma': 0})  # For baseline with no gamma\n",
        "    df['Gamma'] = df['Gamma'].astype(float)\n",
        "\n",
        "    # --- Runtime Plot ---\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    sns.lineplot(data=df, x='Gamma', y='Runtime(s)', hue='Method', marker='o')\n",
        "    plt.title(\"Runtime vs Gamma Threshold\", fontsize=13, fontweight='bold')\n",
        "    plt.xlabel(\"Gamma (QR Threshold)\")\n",
        "    plt.ylabel(\"Runtime (seconds)\")\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    plt.legend(title=\"Method\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # --- Feature Retention Plot ---\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    sns.lineplot(data=df, x='Gamma', y='#Features', hue='Method', marker='s')\n",
        "    plt.title(\"Feature Retention vs Gamma Threshold\", fontsize=13, fontweight='bold')\n",
        "    plt.xlabel(\"Gamma (QR Threshold)\")\n",
        "    plt.ylabel(\"Number of Features\")\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    plt.legend(title=\"Method\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # --- Results Table ---\n",
        "    print(\"\\nüìã Combined Results Summary:\")\n",
        "    display(df.sort_values(by=['Method', 'Gamma']).reset_index(drop=True))\n",
        "\n",
        "    # Optionally save results\n",
        "    if save_csv:\n",
        "        df.to_csv(\"experiment_results.csv\", index=False)\n",
        "        print(\"üíæ Results saved to 'experiment_results.csv'\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# --- Example Run ---\n",
        "all_results = visualize_results([results_base, results_qr, results_hybrid])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
